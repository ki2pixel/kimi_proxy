# Session 2026-02-14 : Correction du Routing des Mod√®les & Endpoint /models

## R√©sum√©

Correction de l'erreur **"Model/deployment not found for: nvidia/kimi-k2-thinking"** de Continue.dev en ajoutant l'endpoint OpenAI-compatible `GET /models` et en fixant la logique de mapping des noms de mod√®les.

---

## üéØ Probl√®me Identifi√©

### Sympt√¥me
Continue.dev affichait l'erreur :
```
Model/deployment not found for: nvidia/kimi-k2-thinking
```

### Causes Racines

1. **Endpoint `GET /models` manquant** : Continue valide les mod√®les via cet endpoint OpenAI standard, mais le proxy retournait 404

2. **Nettoyage incorrect du nom de mod√®le** : Le proxy utilisait `split('/')` pour retirer le pr√©fixe provider, transformant :
   - `nvidia/kimi-k2-thinking` ‚Üí `kimi-k2-thinking` ‚ùå
   - Au lieu de `moonshotai/kimi-k2-thinking` ‚úÖ (nom attendu par l'API NVIDIA)

---

## ‚úÖ Solutions Impl√©ment√©es

### 1. Endpoint OpenAI-Compatible `GET /models`

**Fichier** : `main.py`

```python
@app.get("/models")
async def openai_models():
    """
    Endpoint OpenAI-compatible GET /models pour validation Continue.dev.
    Retourne la liste des mod√®les au format OpenAI standard.
    """
    models_list = []
    
    for model_key, model_data in MODELS.items():
        # Le nom expos√© est le model_key client (ex: "nvidia/kimi-k2-thinking")
        client_model_id = model_key
        
        models_list.append({
            "id": client_model_id,
            "object": "model",
            "created": 1677610602,
            "owned_by": model_data.get("provider", "unknown"),
            "permission": [],
            "root": client_model_id,
            "parent": None
        })
    
    return {
        "object": "list",
        "data": models_list
    }
```

**Format de r√©ponse** (OpenAI-compatible) :
```json
{
    "object": "list",
    "data": [
        {
            "id": "nvidia/kimi-k2-thinking",
            "object": "model",
            "created": 1677610602,
            "owned_by": "nvidia",
            "root": "nvidia/kimi-k2-thinking"
        }
    ]
}
```

### 2. Correction de la Logique de Mapping

**Fichier** : `main.py` (dans `proxy_chat()`)

**Avant** (bug) :
```python
model_name = body_json.get('model', '')
if '/' in model_name:
    clean_model = model_name.split('/', 1)[1]  # ‚Üí "kimi-k2-thinking" ‚ùå
    body_json['model'] = clean_model
```

**Apr√®s** (corrig√©) :
```python
model_name = body_json.get('model', '')
original_model = model_name

# Utilise le mod√®le mapp√© depuis la config si disponible
if model_name in MODELS:
    mapped_model = MODELS[model_name].get('model', model_name)
    body_json['model'] = mapped_model
    print(f"üìù Mod√®le mapp√©: {original_model} ‚Üí {mapped_model}")
elif '/' in model_name:
    # Fallback: retire le pr√©fixe provider (ancien comportement)
    clean_model = model_name.split('/', 1)[1]
    body_json['model'] = clean_model
    print(f"üìù Mod√®le nettoy√© (fallback): {original_model} ‚Üí {clean_model}")
```

**Mapping des mod√®les** (dans `config.toml`) :
```toml
[models."nvidia/kimi-k2-thinking"]
provider = "nvidia"
model = "moonshotai/kimi-k2-thinking"  # ‚Üê Nom attendu par l'API NVIDIA
max_context_size = 262144
```

---

## üß™ Tests de Validation

### Test 1 : Endpoint /models

```bash
curl -s http://localhost:8000/models | python3 -m json.tool
```

**R√©sultat** :
```json
{
    "object": "list",
    "data": [
        {"id": "kimi-code/kimi-for-coding", "owned_by": "managed:kimi-code"},
        {"id": "nvidia/kimi-k2.5", "owned_by": "nvidia"},
        {"id": "nvidia/kimi-k2-thinking", "owned_by": "nvidia"},
        {"id": "mistral/codestral-2501", "owned_by": "mistral"}
        // ... 16 autres mod√®les
    ]
}
```

‚úÖ **20 mod√®les expos√©s** au format OpenAI-compatible

### Test 2 : Requ√™te Proxy avec Mapping

```bash
curl -s -X POST http://localhost:8000/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer dummy-key" \
  -d '{
    "model": "nvidia/kimi-k2-thinking",
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 10,
    "stream": false
  }' | python3 -m json.tool | grep model
```

**R√©sultat** :
```json
"model": "moonshotai/kimi-k2-thinking"
```

‚úÖ **Mapping correct** : `nvidia/kimi-k2-thinking` ‚Üí `moonshotai/kimi-k2-thinking`

### Test 3 : Logs du Serveur

```
üìù Mod√®le mapp√©: nvidia/kimi-k2-thinking ‚Üí moonshotai/kimi-k2-thinking
üîë Cl√© API nvidia inject√©e: nvapi-5kNTmO...
üåê Header Host mis √† jour: integrate.api.nvidia.com
üîÑ Proxy vers nvidia (openai): https://integrate.api.nvidia.com/v1
‚úÖ Vrais tokens re√ßus (non-stream): {'prompt_tokens': 8, 'completion_tokens': 10}
```

‚úÖ **Requ√™te transmise correctement** √† l'API NVIDIA

---

## üìã Coh√©rence des Configurations

| Fichier | Mod√®le NVIDIA K2 Thinking | Usage |
|---------|---------------------------|-------|
| `config.toml` (proxy) | `model = "moonshotai/kimi-k2-thinking"` | API destination |
| `config.yaml` (proxy) | `model: nvidia/kimi-k2-thinking` | Routing via proxy |
| `config_continue_optimis√©.yaml` | `model: moonshotai/kimi-k2-thinking` | Direct NVIDIA (sans proxy) |

---

## üîß D√©tails Techniques

### Pourquoi le Mapping est N√©cessaire

Les providers utilisent diff√©rents formats de noms de mod√®les :

| Provider | Format Client | Format API |
|----------|---------------|------------|
| NVIDIA | `nvidia/kimi-k2-thinking` | `moonshotai/kimi-k2-thinking` |
| NVIDIA | `nvidia/kimi-k2.5` | `moonshotai/kimi-k2.5` |
| SiliconFlow | `siliconflow/qwen3-32b` | `Qwen/Qwen3-32B` |
| SiliconFlow | `siliconflow/deepseek-v3.2` | `deepseek-ai/DeepSeek-V3.2` |
| Groq | `groq/gpt-oss-120b` | `openai/gpt-oss-120b` |

Le dictionnaire `MODELS` dans `main.py` sert de table de correspondance entre :
- **Cl√©** : identifiant client (ce que Continue envoie)
- **Valeur `model`** : nom r√©el attendu par l'API provider

### Fallback S√©curis√©

Si un mod√®le n'est pas trouv√© dans `MODELS`, le proxy utilise le fallback `split('/')` pour compatibilit√© descendante. Cela permet de supporter des mod√®les non configur√©s tout en privil√©giant les mappings explicites.

---

## üìÅ Fichiers Modifi√©s

| Fichier | Changements |
|---------|-------------|
| `main.py` | Ajout endpoint `GET /models`, correction logique mapping dans `proxy_chat()` |

---

## ‚úÖ Checklist

- [x] Endpoint `GET /models` OpenAI-compatible ajout√©
- [x] Mapping des mod√®les corrig√© (utilise `MODELS[client_model]["model"]`)
- [x] Fallback `split('/')` conserv√© pour compatibilit√©
- [x] Tests de validation pass√©s (endpoint + proxy)
- [x] Coh√©rence des configurations v√©rifi√©e
- [x] Documentation mise √† jour (AGENTS.md, README.md)

---

**Date** : 2026-02-14  
**Auteur** : Kimi Code CLI  
**Version** : 3.0.1 Fix Model Routing
