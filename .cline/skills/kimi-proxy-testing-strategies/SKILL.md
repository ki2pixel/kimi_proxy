---\nname: kimi-proxy-testing-strategies\ndescription: Comprehensive testing strategies for Kimi Proxy Dashboard. Use when writing tests, debugging issues, or ensuring system reliability. Covers unit tests, integration, E2E testing, and performance testing with pytest-asyncio.\nlicense: Complete terms in LICENSE.txt\n---\n\n# Kimi Proxy Testing Strategies\n\nThis skill provides comprehensive testing strategies for Kimi Proxy Dashboard.\n\n## Testing Pyramid Structure\n\n### Unit Tests (Bottom Layer - 70% of tests)\n**Purpose**: Test individual functions and classes in isolation\n**Scope**: Single function/method, no external dependencies\n**Tools**: pytest, unittest.mock\n\n### Integration Tests (Middle Layer - 20% of tests)\n**Purpose**: Test component interactions and data flow\n**Scope**: Multiple components, database interactions\n**Tools**: pytest-asyncio, aiosqlite\n\n### End-to-End Tests (Top Layer - 10% of tests)\n**Purpose**: Test complete user workflows\n**Scope**: Full application stack, external APIs\n**Tools**: pytest, httpx, selenium/playwright\n\n## Unit Testing Patterns\n\n### Async Function Testing\n\n```python\n# tests/unit/test_tokens.py\nimport pytest\nfrom src.kimi_proxy.core.tokens import count_tokens\n\nclass TestTokenCounting:\n    @pytest.mark.asyncio\n    async def test_count_tokens_basic(self):\n        \"\"\"Test basic token counting functionality.\"\"\"\n        text = \"Hello world\"\n        tokens = count_tokens(text)\n        assert tokens == 2  # Expected token count\n    \n    @pytest.mark.asyncio\n    async def test_count_tokens_empty(self):\n        \"\"\"Test token counting with empty string.\"\"\"\n        text = \"\"\n        tokens = count_tokens(text)\n        assert tokens == 0\n    \n    @pytest.mark.asyncio\n    async def test_count_tokens_large_text(self):\n        \"\"\"Test token counting with large text.\"\"\"\n        large_text = \"word \" * 1000\n        tokens = count_tokens(large_text)\n        assert tokens > 100  # Should have reasonable token count\n    \n    @pytest.mark.asyncio\n    async def test_count_tokens_special_characters(self):\n        \"\"\"Test token counting with special characters.\"\"\"\n        text = \"Hello, world! üåç ÊµãËØï 123\"\n        tokens = count_tokens(text)\n        assert tokens > 0  # Should handle Unicode\n\n@pytest.fixture\ndef sample_texts():\n    \"\"\"Fixture providing sample texts for testing.\"\"\"\n    return [\n        \"Hello world\",\n        \"This is a test message\",\n        \"Special characters: √†√°√¢√£√§√•√¶√ß√®√©√™√´\",\n        \"Very long text \" * 100\n    ]\n\nclass TestTokenCountingParameterized:\n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"text,expected_min_tokens\", [\n        (\"Hello\", 1),\n        (\"Hello world\", 2),\n        (\"This is a longer sentence with more words\", 8),\n    ])\n    async def test_token_count_ranges(self, text, expected_min_tokens):\n        \"\"\"Test token counting with parameterized inputs.\"\"\"\n        tokens = count_tokens(text)\n        assert tokens >= expected_min_tokens\n```\n\n### Mocking External Dependencies\n\n```python\n# tests/unit/test_api.py\nimport pytest\nfrom unittest.mock import AsyncMock, MagicMock\nfrom src.kimi_proxy.api.routes.sessions import create_session\n\nclass TestSessionAPI:\n    @pytest.fixture\n    def mock_db(self):\n        \"\"\"Mock database connection.\"\"\"\n        mock_conn = AsyncMock()\n        mock_cursor = AsyncMock()\n        mock_conn.cursor.return_value = mock_cursor\n        mock_conn.execute.return_value = mock_cursor\n        mock_conn.__aenter__ = AsyncMock(return_value=mock_conn)\n        mock_conn.__aexit__ = AsyncMock(return_value=None)\n        return mock_conn\n    \n    @pytest.fixture\n    def mock_request(self):\n        \"\"\"Mock FastAPI request object.\"\"\"\n        request = MagicMock()\n        request.json.return_value = {\"name\": \"Test Session\"}\n        return request\n    \n    @pytest.mark.asyncio\n    async def test_create_session_success(self, mock_db, mock_request):\n        \"\"\"Test successful session creation.\"\"\"\n        # Arrange\n        expected_session_id = 123\n        mock_db.execute.return_value.lastrowid = expected_session_id\n        \n        # Act\n        result = await create_session(mock_request, mock_db)\n        \n        # Assert\n        assert result[\"session_id\"] == expected_session_id\n        assert result[\"name\"] == \"Test Session\"\n        mock_db.execute.assert_called_once()\n    \n    @pytest.mark.asyncio\n    async def test_create_session_database_error(self, mock_db, mock_request):\n        \"\"\"Test session creation with database error.\"\"\"\n        # Arrange\n        mock_db.execute.side_effect = Exception(\"Database error\")\n        \n        # Act & Assert\n        with pytest.raises(Exception, match=\"Database error\"):\n            await create_session(mock_request, mock_db)\n```\n\n### Testing Exceptions and Edge Cases\n\n```python\n# tests/unit/test_error_handling.py\nimport pytest\nfrom src.kimi_proxy.core.exceptions import ValidationError, APIError\n\nclass TestErrorHandling:\n    @pytest.mark.asyncio\n    async def test_validation_error_message(self):\n        \"\"\"Test validation error formatting.\"\"\"\n        error = ValidationError(\"Invalid input\", field=\"email\")\n        assert \"Invalid input\" in str(error)\n        assert error.field == \"email\"\n    \n    @pytest.mark.asyncio\n    async def test_api_error_with_details(self):\n        \"\"\"Test API error with additional details.\"\"\"\n        details = {\"provider\": \"kimi\", \"status_code\": 429}\n        error = APIError(\"Rate limit exceeded\", details=details)\n        assert error.details == details\n        assert error.details[\"provider\"] == \"kimi\"\n    \n    @pytest.mark.asyncio\n    @pytest.mark.parametrize(\"input_value,expected_error\", [\n        (None, \"Input cannot be None\"),\n        (\"\", \"Input cannot be empty\"),\n        (\"   \", \"Input cannot be whitespace\"),\n        (123, \"Input must be a string\"),\n    ])\n    async def test_input_validation_edge_cases(self, input_value, expected_error):\n        \"\"\"Test input validation with various edge cases.\"\"\"\n        from src.kimi_proxy.core.validators import validate_string_input\n        \n        with pytest.raises(ValueError, match=expected_error):\n            validate_string_input(input_value)\n```\n\n## Integration Testing Patterns\n\n### Database Integration Tests\n\n```python\n# tests/integration/test_database.py\nimport pytest\nimport aiosqlite\nfrom src.kimi_proxy.core.database import init_database, get_db_connection\n\nclass TestDatabaseIntegration:\n    @pytest.fixture(scope=\"function\")\n    async def test_db(self):\n        \"\"\"Create in-memory database for testing.\"\"\"\n        db = await aiosqlite.connect(\":memory:\")\n        await init_database(db)\n        yield db\n        await db.close()\n    \n    @pytest.mark.asyncio\n    async def test_session_creation_and_retrieval(self, test_db):\n        \"\"\"Test creating and retrieving a session.\"\"\"\n        # Create session\n        await test_db.execute(\n            \"INSERT INTO sessions (name, created_at) VALUES (?, ?)\",\n            (\"Test Session\", \"2024-01-01 00:00:00\")\n        )\n        session_id = test_db.lastrowid\n        \n        # Retrieve session\n        async with test_db.execute(\n            \"SELECT id, name FROM sessions WHERE id = ?\",\n            (session_id,)\n        ) as cursor:\n            row = await cursor.fetchone()\n        \n        assert row[0] == session_id\n        assert row[1] == \"Test Session\"\n    \n    @pytest.mark.asyncio\n    async def test_message_association_with_session(self, test_db):\n        \"\"\"Test that messages are properly associated with sessions.\"\"\"\n        # Create session\n        await test_db.execute(\n            \"INSERT INTO sessions (name) VALUES (?)\",\n            (\"Test Session\",)\n        )\n        session_id = test_db.lastrowid\n        \n        # Add messages\n        messages = [\n            (session_id, \"user\", \"Hello\"),\n            (session_id, \"assistant\", \"Hi there!\"),\n            (session_id, \"user\", \"How are you?\")\n        ]\n        \n        await test_db.executemany(\n            \"INSERT INTO messages (session_id, role, content) VALUES (?, ?, ?)\",\n            messages\n        )\n        \n        # Verify message count\n        async with test_db.execute(\n            \"SELECT COUNT(*) FROM messages WHERE session_id = ?\",\n            (session_id,)\n        ) as cursor:\n            count = await cursor.fetchone()\n        \n        assert count[0] == 3\n```\n\n### API Integration Tests\n\n```python\n# tests/integration/test_api_integration.py\nimport pytest\nfrom httpx import AsyncClient\nfrom src.kimi_proxy.main import app\n\nclass TestAPIIntegration:\n    @pytest.fixture(scope=\"function\")\n    async def client(self):\n        \"\"\"Create test client.\"\"\"\n        async with AsyncClient(app=app, base_url=\"http://testserver\") as client:\n            yield client\n    \n    @pytest.fixture(scope=\"function\")\n    async def test_db(self):\n        \"\"\"Setup test database.\"\"\"\n        # Override database connection for testing\n        from src.kimi_proxy.core.database import set_test_database\n        test_db = await set_test_database()\n        yield test_db\n        # Cleanup after test\n    \n    @pytest.mark.asyncio\n    async def test_create_and_retrieve_session(self, client, test_db):\n        \"\"\"Test full session lifecycle through API.\"\"\"\n        # Create session\n        response = await client.post(\"/api/sessions\", json={\n            \"name\": \"Integration Test Session\"\n        })\n        assert response.status_code == 201\n        \n        session_data = response.json()\n        session_id = session_data[\"id\"]\n        \n        # Retrieve session\n        response = await client.get(f\"/api/sessions/{session_id}\")\n        assert response.status_code == 200\n        \n        retrieved_data = response.json()\n        assert retrieved_data[\"name\"] == \"Integration Test Session\"\n        assert retrieved_data[\"id\"] == session_id\n    \n    @pytest.mark.asyncio\n    async def test_session_with_messages(self, client, test_db):\n        \"\"\"Test session with message interactions.\"\"\"\n        # Create session\n        response = await client.post(\"/api/sessions\", json={\n            \"name\": \"Message Test Session\"\n        })\n        session_id = response.json()[\"id\"]\n        \n        # Add messages\n        messages = [\n            {\"role\": \"user\", \"content\": \"Hello\"},\n            {\"role\": \"assistant\", \"content\": \"Hi there!\"}\n        ]\n        \n        for msg in messages:\n            response = await client.post(f\"/api/sessions/{session_id}/messages\", json=msg)\n            assert response.status_code == 201\n        \n        # Retrieve session with messages\n        response = await client.get(f\"/api/sessions/{session_id}\")\n        assert response.status_code == 200\n        \n        session_data = response.json()\n        assert len(session_data[\"messages\"]) == 2\n        assert session_data[\"messages\"][0][\"content\"] == \"Hello\"\n        assert session_data[\"messages\"][1][\"content\"] == \"Hi there!\"\n```\n\n## End-to-End Testing Patterns\n\n### Full Workflow Testing\n\n```python\n# tests/e2e/test_complete_workflow.py\nimport pytest\nfrom playwright.async_api import async_playwright\n\nclass TestCompleteWorkflow:\n    @pytest.fixture(scope=\"session\")\n    async def browser_context(self):\n        \"\"\"Setup browser context for E2E tests.\"\"\"\n        async with async_playwright() as p:\n            browser = await p.chromium.launch()\n            context = await browser.new_context()\n            yield context\n            await browser.close()\n    \n    @pytest.mark.asyncio\n    async def test_user_creates_session_and_chats(self, browser_context):\n        \"\"\"Test complete user workflow: create session, send messages, view responses.\"\"\"\n        page = await browser_context.new_page()\n        \n        # Navigate to application\n        await page.goto(\"http://localhost:3000\")\n        \n        # Create new session\n        await page.click(\"[data-testid='new-session-button']\")\n        await page.fill(\"[data-testid='session-name-input']\", \"E2E Test Session\")\n        await page.click(\"[data-testid='create-session-submit']\")\n        \n        # Verify session created\n        await page.wait_for_selector(\"[data-testid='session-title']\")\n        title = await page.text_content(\"[data-testid='session-title']\")\n        assert \"E2E Test Session\" in title\n        \n        # Send a message\n        await page.fill(\"[data-testid='message-input']\", \"Hello, can you help me?\")\n        await page.click(\"[data-testid='send-message-button']\")\n        \n        # Wait for response\n        await page.wait_for_selector(\"[data-testid='assistant-message']\")\n        response = await page.text_content(\"[data-testid='assistant-message']\")\n        assert len(response) > 0\n        \n        # Check token counter updated\n        token_display = await page.text_content(\"[data-testid='token-counter']\")\n        assert \"tokens\" in token_display.lower()\n    \n    @pytest.mark.asyncio\n    async def test_error_handling(self, browser_context):\n        \"\"\"Test error handling in UI.\"\"\"\n        page = await browser_context.new_page()\n        await page.goto(\"http://localhost:3000\")\n        \n        # Try to access non-existent session\n        await page.goto(\"http://localhost:3000/session/99999\")\n        \n        # Should show error page\n        error_message = await page.text_content(\"[data-testid='error-message']\")\n        assert \"not found\" in error_message.lower()\n```\n\n### API E2E Testing\n\n```python\n# tests/e2e/test_api_e2e.py\nimport pytest\nfrom httpx import AsyncClient\nfrom src.kimi_proxy.main import app\n\nclass TestAPIE2E:\n    @pytest.fixture(scope=\"function\")\n    async def client(self):\n        \"\"\"Create test client with full application.\"\"\"\n        async with AsyncClient(app=app, base_url=\"http://testserver\") as client:\n            yield client\n    \n    @pytest.mark.asyncio\n    async def test_chat_completion_workflow(self, client):\n        \"\"\"Test complete chat completion workflow.\"\"\"\n        # Create session\n        response = await client.post(\"/api/sessions\", json={\n            \"name\": \"E2E Chat Test\"\n        })\n        assert response.status_code == 201\n        session_id = response.json()[\"id\"]\n        \n        # Send chat message\n        chat_request = {\n            \"messages\": [\n                {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n            ],\n            \"model\": \"kimi-for-coding\",\n            \"stream\": False\n        }\n        \n        response = await client.post(f\"/api/sessions/{session_id}/chat\", json=chat_request)\n        assert response.status_code == 200\n        \n        chat_response = response.json()\n        assert \"choices\" in chat_response\n        assert len(chat_response[\"choices\"]) > 0\n        assert \"Paris\" in chat_response[\"choices\"][0][\"message\"][\"content\"]\n        \n        # Verify session updated\n        response = await client.get(f\"/api/sessions/{session_id}\")\n        session_data = response.json()\n        assert session_data[\"message_count\"] == 2  # user + assistant\n        assert session_data[\"token_count\"] > 0\n```\n\n## Performance Testing\n\n### Load Testing Setup\n\n```python\n# tests/performance/test_load.py\nimport pytest\nimport asyncio\nimport time\nfrom httpx import AsyncClient\nfrom statistics import mean, median\n\nclass TestLoadPerformance:\n    @pytest.fixture(scope=\"function\")\n    async def client(self):\n        \"\"\"Create client for load testing.\"\"\"\n        async with AsyncClient(\n            app=app,\n            base_url=\"http://testserver\",\n            timeout=30.0\n        ) as client:\n            yield client\n    \n    @pytest.mark.asyncio\n    async def test_concurrent_session_creation(self, client):\n        \"\"\"Test creating multiple sessions concurrently.\"\"\"\n        async def create_session(n):\n            start_time = time.time()\n            response = await client.post(\"/api/sessions\", json={\n                \"name\": f\"Load Test Session {n}\"\n            })\n            end_time = time.time()\n            return response.status_code == 201, end_time - start_time\n        \n        # Create 50 sessions concurrently\n        tasks = [create_session(i) for i in range(50)]\n        results = await asyncio.gather(*tasks)\n        \n        successes = [r[0] for r in results]\n        response_times = [r[1] for r in results]\n        \n        # Assert performance\n        success_rate = sum(successes) / len(successes)\n        assert success_rate >= 0.95  # 95% success rate\n        \n        avg_response_time = mean(response_times)\n        assert avg_response_time < 2.0  # Under 2 seconds average\n        \n        p95_response_time = sorted(response_times)[int(len(response_times) * 0.95)]\n        assert p95_response_time < 5.0  # 95th percentile under 5 seconds\n    \n    @pytest.mark.asyncio\n    async def test_chat_throughput(self, client):\n        \"\"\"Test chat message throughput under load.\"\"\"\n        # Create session first\n        response = await client.post(\"/api/sessions\", json={\n            \"name\": \"Throughput Test\"\n        })\n        session_id = response.json()[\"id\"]\n        \n        async def send_message(n):\n            start_time = time.time()\n            response = await client.post(f\"/api/sessions/{session_id}/chat\", json={\n                \"messages\": [{\"role\": \"user\", \"content\": f\"Test message {n}\"}],\n                \"model\": \"kimi-for-coding\",\n                \"max_tokens\": 50\n            })\n            end_time = time.time()\n            return response.status_code == 200, end_time - start_time\n        \n        # Send 20 messages concurrently\n        tasks = [send_message(i) for i in range(20)]\n        results = await asyncio.gather(*tasks)\n        \n        successes = [r[0] for r in results]\n        response_times = [r[1] for r in results]\n        \n        success_rate = sum(successes) / len(successes)\n        assert success_rate >= 0.90  # 90% success rate for chat\n        \n        avg_response_time = mean(response_times)\n        assert avg_response_time < 10.0  # Under 10 seconds for chat responses\n```\n\n## Test Configuration and Fixtures\n\n### Pytest Configuration\n\n```ini\n# pytest.ini\n[tool:pytest.ini_options]\nminversion = \"6.0\"\naddopts =\n    -v\n    --tb=short\n    --strict-markers\n    --disable-warnings\n    --asyncio-mode=auto\nmarkers =\n    slow: marks tests as slow (deselect with '-m \"not slow\"')\n    integration: marks tests as integration tests\n    e2e: marks tests as end-to-end tests\n    unit: marks tests as unit tests\nasyncio_mode = auto\n```\n\n### Shared Test Fixtures\n\n```python\n# tests/conftest.py\nimport pytest\nimport asyncio\nfrom httpx import AsyncClient\nfrom src.kimi_proxy.main import app\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    \"\"\"Create event loop for async tests.\"\"\"\n    loop = asyncio.get_event_loop_policy().new_event_loop()\n    yield loop\n    loop.close()\n\n@pytest.fixture(scope=\"function\")\nasync def client():\n    \"\"\"Create HTTP client for API tests.\"\"\"\n    async with AsyncClient(app=app, base_url=\"http://testserver\") as client:\n        yield client\n\n@pytest.fixture(scope=\"function\")\nasync def test_db():\n    \"\"\"Create test database.\"\"\"\n    from src.kimi_proxy.core.database import create_test_database\n    db = await create_test_database()\n    yield db\n    await db.close()\n\n@pytest.fixture(scope=\"function\")\nasync def sample_session(test_db):\n    \"\"\"Create sample session for testing.\"\"\"\n    await test_db.execute(\n        \"INSERT INTO sessions (name, created_at) VALUES (?, ?)\",\n        (\"Test Session\", \"2024-01-01 00:00:00\")\n    )\n    session_id = test_db.lastrowid\n    return session_id\n```\n\n## CI/CD Integration\n\n### GitHub Actions Test Workflow\n\n```yaml\n# .github/workflows/test.yml\nname: Tests\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.10, 3.11]\n    \n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n    \n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install -r requirements-dev.txt\n    \n    - name: Run unit tests\n      run: |\n        PYTHONPATH=src python -m pytest tests/unit/ -v --cov=src/kimi_proxy\n    \n    - name: Run integration tests\n      run: |\n        PYTHONPATH=src python -m pytest tests/integration/ -v\n    \n    - name: Run E2E tests\n      run: |\n        PYTHONPATH=src python -m pytest tests/e2e/ -v\n    \n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n```\n",      "size": 22339,
      "size_readable": "21.82 KB",
      "modified": "2026-02-19T01:32:23.673Z",
      "created": "2026-02-19T01:32:23.673Z",
      "extension": ".md",
      "mime_type": "text/markdown",
      "encoding": "utf-8",
      "bytes_read": 0,
      "start_offset": 0,
      "end_offset": 22339,
      "is_complete": true,
      "has_more": false,
      "continuation_token": null,
      "auto_continued": true,
      "index": 3
    },
    {
      "path": "/home/kidpixel/kimi-proxy/.cline/skills/shrimp-task-manager/SKILL.md",
      "name": "SKILL.md",
      "type": "text",
      "content": "---\nname: shrimp-task-manager\ndescription: Expert en gestion de t√¢ches via les outils shrimp-task-manager. G√®re les backlogs, roadmaps et analyse de complexit√© pour transformer les demandes complexes en plans d'action structur√©s.\n---\n\n# Shrimp Task Manager\n\n> **Expertise** : Planification de projet, gestion de backlog, analyse de complexit√©, transformation PRD en t√¢ches ex√©cutables via outils shrimp-task-manager.\n\n## Quick Start\n\n### Mental Model\n\nShrimp Task Manager transforme une demande complexe en un plan structur√© utilisant les outils shrimp-task-manager pour :\n- Parser les PRD (Product Requirements Documents)\n- Analyser la complexit√© technique\n- G√©n√©rer des backlogs prioris√©s\n- Cr√©er des roadmaps temporaires\n\n### Workflow obligatoire\n\n1. **Plan Task** : Utiliser `plan_task` pour recevoir des conseils de planification structur√©s\n2. **Analyze Task** : `analyze_task` pour analyser les exigences en profondeur et √©valuer la faisabilit√©\n3. **Reflect Task** : `reflect_task` pour r√©viser l'analyse et identifier les optimisations\n4. **Split Tasks** : `split_tasks` pour d√©composer en sous-t√¢ches ind√©pendantes avec d√©pendances\n5. **Execute Task** : `execute_task` pour ex√©cuter les t√¢ches avec guidance √©tape par √©tape\n6. **Verify Task** : `verify_task` pour valider avec scoring selon les crit√®res de qualit√©\n\n### Patterns d'utilisation\n\n#### Pour un nouveau projet\n\n```bash\n# Planification initiale\nplan_task(description=\"Description compl√®te du projet\", requirements=\"Exigences techniques\", existingTasksReference=false)\n\n# Analyse technique\nanalyze_task(summary=\"R√©sum√© structur√© du projet\", initialConcept=\"Solution technique propos√©e\", previousAnalysis=\"\")\n\n# D√©composition\nsplit_tasks(updateMode=\"clearAllTasks\", tasksRaw=\"[liste des t√¢ches structur√©es]\")\n\n# Ex√©cution guid√©e\nexecute_task(taskId=\"ID de la t√¢che\")\n\n# V√©rification\nverify_task(taskId=\"ID de la t√¢che\", summary=\"R√©sum√©\", score=85)\n```\n\n#### Pour une √©volution de fonctionnalit√©\n\n```bash\n# Analyse d'impact\nanalyze_task(summary=\"Impact de la fonctionnalit√©\", initialConcept=\"Solution propos√©e\", previousAnalysis=\"\")\n\n# D√©composition en sous-t√¢ches\nsplit_tasks(updateMode=\"append\", tasksRaw=\"[sous-t√¢ches]\")\n\n# Ex√©cution it√©rative\nexecute_task(taskId=\"ID sous-t√¢che\")\nverify_task(taskId=\"ID sous-t√¢che\", summary=\"Validation\", score=90)\n```\n\n## Production-safe patterns\n\n### Validation avant ex√©cution\n\nToujours analyser avec `analyze_task` avant de commencer le d√©veloppement :\n\n```bash\n# √âtape 1 : Analyse de complexit√©\nanalyze_task(summary=\"Analyse t√¢che\", initialConcept=\"Solution\", previousAnalysis=\"\")\n\n# √âtape 2 : Revue critique\nreflect_task(summary=\"R√©sum√©\", analysis=\"R√©sultats analyse\")\n\n# √âtape 3 : D√©composition\nsplit_tasks(updateMode=\"overwrite\", tasksRaw=\"[t√¢ches]\")\n```\n\n### Gestion des priorit√©s\n\nShrimp Task Manager g√©n√®re automatiquement des priorit√©s bas√©es sur :\n- D√©pendances critiques\n- Impact utilisateur\n- Risque technique\n- Effort estim√©\n\n### Integration avec Memory Bank\n\nUtilise `fast_read_file` pour charger le contexte avant analyse :\n```bash\n# Charger le contexte actif\nfast_read_file path=\"/home/kidpixel/kimi-proxy/memory-bank/activeContext.md\"\n\n# Lancer l'analyse\nanalyze_task(summary=\"Contexte charg√©\", initialConcept=\"Solution\", previousAnalysis=\"\")\n```\n\n## Common gotchas\n\n### T√¢ches trop complexes\n\n- Utiliser `split_tasks` avec les r√®gles de granularit√© (1-2 jours par t√¢che)\n- √âviter plus de 10 sous-t√¢ches d'un coup\n- Respecter les 3 niveaux maximum de profondeur\n\n### D√©pendances manqu√©es\n\n- Sp√©cifier explicitement les d√©pendances dans `split_tasks`\n- V√©rifier le graphe de d√©pendances g√©n√©r√© automatiquement\n- Utiliser `list_tasks` pour visualiser les encha√Ænements\n\n### Scores de v√©rification faibles\n\n- Relire les crit√®res dans `get_task_detail`\n- Utiliser `update_task` pour am√©liorer la t√¢che avant reverification\n- Scores < 80 n√©cessitent correction obligatoire\n\n## API Reference\n\n### Outils principaux\n\n- `plan_task(description, requirements, existingTasksReference)` : Planification de t√¢ches avec guidance structur√©. Arguments : description(string), requirements(string), existingTasksReference(boolean)\n- `analyze_task(summary, initialConcept, previousAnalysis)` : Analyse approfondie des exigences. Arguments : summary(string), initialConcept(string), previousAnalysis(string)\n- `reflect_task(summary, analysis)` : Revue critique des analyses. Arguments : summary(string), analysis(string)\n- `split_tasks(updateMode, tasksRaw, globalAnalysisResult)` : D√©composition en sous-t√¢ches. Arguments : updateMode(string), tasksRaw(string), globalAnalysisResult(string)\n- `list_tasks(status)` : Liste structur√©e des t√¢ches. Arguments : status(string)\n- `execute_task(taskId)` : Ex√©cution guid√©e d'une t√¢che. Arguments : taskId(string)\n- `verify_task(taskId, summary, score)` : V√©rification et scoring. Arguments : taskId(string), summary(string), score(number)\n- `delete_task(taskId)` : Suppression t√¢ches incompl√®tes. Arguments : taskId(string)\n- `clear_all_tasks(confirm)` : Nettoyage avec sauvegarde. Arguments : confirm(boolean)\n- `update_task(taskId, name, description, ...)` : Mise √† jour contenu. Arguments : taskId(string) + champs optionnels\n- `query_task(query, isId, page, pageSize)` : Recherche de t√¢ches. Arguments : query(string), isId(boolean), page(integer), pageSize(integer)\n- `get_task_detail(taskId)` : D√©tails complets. Arguments : taskId(string)\n- `process_thought(thought, thought_number, total_thoughts, next_thought_needed, stage, tags, axioms_used, assumptions_challenged)` : Pens√©e flexible. Arguments : thought(string) + m√©tadonn√©es\n- `init_project_rules()` : Initialisation standards projet. Arguments : aucun\n- `research_mode(topic, previousState, currentState, nextSteps)` : Mode recherche. Arguments : topic(string) + √©tats\n\n### Options avanc√©es\n\n- `updateMode` dans `split_tasks` : 'append', 'overwrite', 'selective', 'clearAllTasks'\n- `status` dans `list_tasks` : 'all', ou statut sp√©cifique\n- `stage` dans `process_thought` : Problem Definition, Information Gathering, Analysis, etc.\n\n## Debugging checklist\n\n- Confirmer que les t√¢ches respectent la granularit√© (1-2 jours)\n- V√©rifier que `split_tasks` g√©n√®re < 10 sous-t√¢ches\n- Utiliser `verify_task` avec score ‚â• 80\n- Employer `process_thought` pour la r√©flexion complexe\n- `research_mode` pour les recherches techniques\n\n## When to use this skill\n\n- **Nouveaux projets** : Quand vous avez un PRD ou des sp√©cifications\n- **Fonctionnalit√©s complexes** : Quand l'impact d√©passe un simple fichier\n- **Planification sprint** : Pour organiser le travail sur plusieurs semaines\n- **Analyse d'impact** : Quand une modification affecte plusieurs composants\n- **D√©composition technique** : Pour transformer une id√©e en t√¢ches ex√©cutables\n\n## Integration patterns\n\n### Avec Sequential Thinking\n\nUtilise `process_thought` avec `sequentialthinking-tools` pour valider la logique de d√©composition et assurer la coh√©rence.\n\n### Avec Fast Filesystem\n\nUtilise `edit_file` pour impl√©menter les t√¢ches g√©n√©r√©es par Shrimp Task Manager de mani√®re chirurgicale.\n\n### Avec JSON Query\n\nUtilise `json_query_jsonpath` pour extraire les donn√©es de configuration des PRD au format JSON.",
      "size": 7309,
      "size_readable": "7.14 KB",
      "modified": "2026-02-22T12:09:56.378Z",
      "created": "2026-02-18T21:16:23.222Z",
      "extension": ".md",
      "mime_type": "text/markdown",
      "encoding": "utf-8",
      "bytes_read": 7309,
      "start_offset": 0,
      "end_offset": 7309,
      "is_complete": true,
      "has_more": false,
      "continuation_token": null,
      "auto_continued": true,
      "index": 4
    }
  ],
  "failed_files": [],
  "continuation_data": null,
  "continuation_guide": null,
  "performance": {
    "parallel_read": true,
    "chunk_size_mb": 1,
    "auto_continue_enabled": true,
    "max_file_size_limit_mb": 5
  },
  "timestamp": "2026-02-23T13:45:50.988Z"
}

Full output saved to: /tmp/windsurf/mcp_output_2fd6d82c8b6b3d85.txt