# Session 2026-02-14 : Extension Multi-Provider & Corrections de Calculs

## R√©sum√©

Extension du Kimi Proxy Dashboard pour supporter **8 providers** et **20+ mod√®les**, avec corrections des incoh√©rences de calcul de tokens et de contexte max.

---

## üéØ Objectifs

1. √âtendre le support √† tous les mod√®les utilis√©s par l'extension Continue
2. Corriger les incoh√©rences de contexte max (ex: Mistral affichait 262K au lieu de 131K)
3. Corriger les calculs cumulatifs de tokens
4. Am√©liorer l'UI du dashboard avec s√©lection granulaire des mod√®les

---

## ‚úÖ Providers Ajout√©s

| Provider | Type | Mod√®les | Contexte |
|----------|------|---------|----------|
| üåô **Kimi Code** | kimi | 1 | 256K |
| üü¢ **NVIDIA** | openai | 2 | 256K |
| üî∑ **Mistral** | openai | 4 | 32K-256K |
| üîÄ **OpenRouter** | openai | 1 | 128K |
| üíß **SiliconFlow** | openai | 2 | 131K-164K |
| ‚ö° **Groq** | openai | 3 | 131K |
| üß† **Cerebras** | openai | 3 | 64K-65K |
| üíé **Gemini** | gemini | 4 | 1M |

---

## üîß Changements Techniques

### Backend (`main.py`)

#### 1. Nouvelle colonne `model` dans la base de donn√©es
```python
# Migration ajout√©e dans init_database()
try:
    cursor.execute("ALTER TABLE sessions ADD COLUMN model TEXT")
    conn.commit()
    print("   Migration: colonne 'model' ajout√©e √† sessions")
except sqlite3.OperationalError:
    pass  # Colonne existe d√©j√†
```

#### 2. Correction de `get_max_context_for_session()`
```python
def get_max_context_for_session(session: dict) -> int:
    """R√©cup√®re le contexte max pour une session bas√© sur son provider.
    
    Si un mod√®le sp√©cifique est stock√© dans la session, utilise son contexte.
    Sinon, utilise le contexte le plus petit parmi les mod√®les du provider.
    """
    if not session:
        return DEFAULT_MAX_CONTEXT
    
    provider_key = session.get("provider", DEFAULT_PROVIDER)
    model_key = session.get("model")  # Mod√®le sp√©cifique si disponible
    
    # Si un mod√®le sp√©cifique est stock√©, utilise son contexte
    if model_key and model_key in MODELS:
        return MODELS[model_key].get("max_context_size", DEFAULT_MAX_CONTEXT)
    
    # Sinon, trouve le contexte le plus petit parmi les mod√®les du provider
    min_context = None
    for mk, model in MODELS.items():
        if model.get("provider") == provider_key:
            ctx = model.get("max_context_size", DEFAULT_MAX_CONTEXT)
            if min_context is None or ctx < min_context:
                min_context = ctx
    
    return min_context if min_context else DEFAULT_MAX_CONTEXT
```

#### 3. Correction de `get_session_total_tokens()`
```python
def get_session_total_tokens(session_id: int) -> dict:
    """Calcule le total cumul√© des tokens pour une session.
    
    Logique:
    - Input: Somme des prompt_tokens (r√©els) sinon estimated_tokens
    - Output: Somme des completion_tokens (r√©els)
    - Total: Input + Output
    """
    conn = get_db()
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT estimated_tokens, prompt_tokens, completion_tokens, is_estimated
        FROM metrics WHERE session_id = ? ORDER BY timestamp ASC
    """, (session_id,))
    
    rows = cursor.fetchall()
    conn.close()
    
    total_input = 0
    total_output = 0
    
    for row in rows:
        estimated = row[0] or 0
        prompt = row[1] or 0
        completion = row[2] or 0
        
        # Pour l'input: utilise prompt_tokens si disponible, sinon estimated_tokens
        if prompt > 0:
            total_input += prompt
        else:
            total_input += estimated
        
        # Pour l'output: toujours completion_tokens
        total_output += completion
    
    return {
        "input_tokens": total_input,
        "output_tokens": total_output,
        "total_tokens": total_input + total_output
    }
```

#### 4. Support Non-Streaming
Ajout d'une branche sp√©cifique dans `proxy_chat()` pour traiter les r√©ponses JSON compl√®tes (sans SSE) et extraire les vrais tokens.

#### 5. Nouveaux Endpoints
- `GET /api/models` : Liste tous les mod√®les avec m√©tadonn√©es
- `POST /api/sessions` : Accepte maintenant un param√®tre `model` optionnel

---

### Configuration (`config.toml`)

Ajout de tous les providers et mod√®les :

```toml
# === Kimi Code Officiel ===
[models."kimi-code/kimi-for-coding"]
provider = "managed:kimi-code"
model = "kimi-for-coding"
max_context_size = 262144

# === NVIDIA ===
[models."nvidia/kimi-k2.5"]
provider = "nvidia"
model = "moonshotai/kimi-k2.5"
max_context_size = 262144

[models."nvidia/kimi-k2-thinking"]
provider = "nvidia"
model = "moonshotai/kimi-k2-thinking"
max_context_size = 262144

# === Mistral ===
[models."mistral/codestral-2501"]
provider = "mistral"
model = "codestral-2501"
max_context_size = 262144

[models."mistral/mistral-large-2411"]
provider = "mistral"
model = "mistral-large-2411"
max_context_size = 131072

[models."mistral/pixtral-large-2411"]
provider = "mistral"
model = "pixtral-large-2411"
max_context_size = 131072

[models."mistral/ministral-8b-2410"]
provider = "mistral"
model = "ministral-8b-2410"
max_context_size = 32768

# ... (autres providers)

# === Providers Configuration ===
[providers.mistral]
type = "openai"
base_url = "https://api.mistral.ai/v1"
api_key = "..."

[providers.openrouter]
type = "openai"
base_url = "https://openrouter.ai/api/v1"
api_key = "..."

[providers.siliconflow]
type = "openai"
base_url = "https://api.siliconflow.cn/v1"
api_key = "..."

[providers.groq]
type = "openai"
base_url = "https://api.groq.com/openai/v1"
api_key = "..."

[providers.cerebras]
type = "openai"
base_url = "https://api.cerebras.ai/v1"
api_key = "..."

[providers.gemini]
type = "gemini"
base_url = "https://generativelanguage.googleapis.com/v1beta"
api_key = "..."
```

---

### Frontend (`static/index.html`)

#### Nouveau Modal de Cr√©ation de Session
- **Filtre de recherche** en temps r√©el
- **Affichage group√©** par provider avec ic√¥nes color√©es
- **Grille de mod√®les** avec indicateurs de capacit√©s
- **Format du contexte** affich√© en K/M
- **Bouton d√©sactiv√©** tant qu'aucun mod√®le n'est s√©lectionn√©

#### Indicateurs de Capacit√©s
- üîß `tool_use` - Support des outils (MCP)
- üß† `thinking` - Mode r√©flexion
- üëÅÔ∏è `vision` - Support vision
- üñºÔ∏è `multimodal` - Support multimodal
- ‚ö° `autocomplete` - Autocompl√©tion
- üí° `reasoning` - Raisonnement avanc√©
- üíª `coding` - Optimis√© codage
- üöÄ `ultra_fast` - Ultra rapide

#### Couleurs des Providers
- üåô Kimi Code: `purple`
- üü¢ NVIDIA: `green`
- üî∑ Mistral: `blue`
- üîÄ OpenRouter: `orange`
- üíß SiliconFlow: `cyan`
- ‚ö° Groq: `yellow`
- üß† Cerebras: `red`
- üíé Gemini: `indigo`

---

## üêõ Corrections de Bugs

### Bug 1: Contexte Max Incorrect
**Sympt√¥me**: La jauge affichait 262144 pour Mistral Large (devrait √™tre 131072)

**Cause**: `get_max_context_for_session()` prenait le premier mod√®le du provider

**Solution**: 
- Stockage du mod√®le sp√©cifique dans la session
- Si pas de mod√®le stock√©, utilise le plus petit contexte (approche conservatrice)

### Bug 2: Calculs Cumulatifs Incorrects
**Sympt√¥me**: Total affich√© 146217 au lieu du vrai total (~67062)

**Cause**: `get_session_total_tokens()` utilisait `estimated_tokens` au lieu de `prompt_tokens`

**Solution**: Refonte compl√®te de la logique:
- Input: somme des `prompt_tokens` (r√©els) ou `estimated_tokens`
- Output: somme des `completion_tokens`
- Total: input + output

### Bug 3: Tokens R√©els Non Extraits (Non-Streaming)
**Sympt√¥me**: Les m√©triques restaient en mode "ESTIM√â" m√™me apr√®s r√©ponse

**Cause**: Le code ne g√©rait que le format SSE (streaming)

**Solution**: Ajout d'une branche pour traiter les r√©ponses JSON compl√®tes

---

## üß™ Tests Effectu√©s

### Test 1: Cr√©ation Session Mistral
```bash
curl -X POST http://localhost:8000/api/sessions \
  -H "Content-Type: application/json" \
  -d '{"name":"Test","provider":"mistral","model":"mistral/mistral-large-2411"}'

# R√©sultat:
# Max Context: 131072  ‚úÖ
```

### Test 2: Calculs de Tokens
```bash
# Apr√®s 2 requ√™tes:
# Input: 16  ‚úÖ (prompt_tokens cumul√©s)
# Output: 342  ‚úÖ (completion_tokens)
# Total: 358  ‚úÖ (input + output)
```

### Test 3: Extraction Tokens R√©els
```bash
curl -X POST http://localhost:8000/chat/completions \
  -d '{"model":"mistral/mistral-large-2411","messages":[...],"stream":false}'

# La m√©trique passe bien de "ESTIM√â" √† "R√âEL"
```

---

## üìÅ Fichiers Modifi√©s

| Fichier | Changements |
|---------|-------------|
| `main.py` | Corrections calculs, support multi-provider, endpoints `/api/models`, gestion non-streaming |
| `config.toml` | 8 providers, 20 mod√®les |
| `config.yaml` | Configuration Continue synchronis√©e |
| `static/index.html` | Nouveau modal, filtres, indicateurs de capacit√©s |
| `AGENTS.md` | Documentation agent mise √† jour |
| `README.md` | Documentation utilisateur mise √† jour |

---

## üìù Notes pour le D√©veloppement Futur

### Rate Limiting par Provider
Actuellement, le rate limiting est global (40 RPM par d√©faut). √Ä terme, on pourrait impl√©menter des limites sp√©cifiques par provider :

```python
RATE_LIMITS = {
    "nvidia": 40,
    "mistral": 60,
    "groq": 100,
    # ...
}
```

### Support Gemini Natif
Le support Gemini est partiel (conversion OpenAI‚ÜíGemini basique). Pour une meilleure compatibilit√©, envisager :
- Mapping complet des param√®tres
- Gestion des formats de r√©ponse diff√©renci√©s
- Support des fonctionnalit√©s sp√©cifiques Gemini (tools, etc.)

### Historique des Sessions
Actuellement, le max_context est stock√© √† la cr√©ation. Si un utilisateur change de mod√®le dans Continue sans cr√©er une nouvelle session, le contexte max reste celui du mod√®le initial.

---

## ‚úÖ Checklist

- [x] Tous les providers configur√©s dans `config.toml`
- [x] Tous les mod√®les avec contexte correct
- [x] Endpoint `/api/models` fonctionnel
- [x] Endpoint `/api/providers` enrichi (ic√¥nes, couleurs)
- [x] Modal UI avec s√©lection granulaire
- [x] Filtre de recherche fonctionnel
- [x] Indicateurs de capacit√©s affich√©s
- [x] Calculs cumulatifs corrig√©s
- [x] Extraction tokens r√©els (streaming + non-streaming)
- [x] Contexte max correct par mod√®le
- [x] Tests de validation pass√©s

---

**Date**: 2026-02-14
**Auteur**: Kimi Code CLI
**Version**: 3.0.0 Multi-Provider
