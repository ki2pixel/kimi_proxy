# ğŸš€ Kimi Proxy Dashboard

**TL;DR**: Je construis un proxy transparent qui me fait Ã©conomiser 20-40% de tokens LLM tout en me montrant exactement ce que je consomme en temps rÃ©el.

J'Ã©tais fatiguÃ© de payer $30/mois pour des services de transcription alors que les APIs me coÃ»taient $0.36/heure. Avec Continue.dev dans PyCharm, je voyais bien mes tokens s'envoler, mais je ne savais pas combien ni pourquoi. Pire encore, je devais choisir entre perdre mon contexte ou payer encore plus.

Alors j'ai construit Kimi Proxy Dashboard. C'est comme avoir un compteur intelligent entre mon IDE et les APIs LLM; il intercepte tout, compte prÃ©cisÃ©ment les tokens avec Tiktoken, et me montre exactement oÃ¹ je dÃ©pense. Le meilleur? Il compresse automatiquement les conversations verbeuses pour me faire Ã©conomiser jusqu'Ã  40% de tokens sans que je perde l'essentiel.

![Dashboard](https://img.shields.io/badge/Dashboard-Live-success?style=for-the-badge)
![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![SQLite](https://img.shields.io/badge/SQLite-07405E?style=for-the-badge&logo=sqlite&logoColor=white)
![WebSocket](https://img.shields.io/badge/WebSocket-Realtime-blue?style=for-the-badge)
![Log Watcher](https://img.shields.io/badge/Log%20Watcher-PyCharm-purple?style=for-the-badge)
![Architecture](https://img.shields.io/badge/Architecture-Modulaire-orange?style=for-the-badge)

## Ce que Ã§a rÃ©sout vraiment

### âŒ Avant : La boÃ®te noire des tokens
- Pas de visibilitÃ© sur ma consommation rÃ©elle
- Contexte qui disparaÃ®t au milieu d'une conversation importante  
- Factures API qui montent sans comprendre pourquoi
- Perte de productivitÃ© Ã  devoir recrÃ©er le contexte

### âœ… AprÃ¨s : Le contrÃ´le total
- **Jauge visuelle instantanÃ©e** : Je vois exactement oÃ¹ j'en suis (Vert â†’ Jaune â†’ Rouge)
- **Ã‰conomie automatique** : Le sanitizer masque les messages tools/console verbeux
- **Compression intelligente** : Un bouton d'urgence quand je dÃ©passe 85% du contexte
- **Historique complet** : Chaque token comptÃ© avec sa source (Proxy, Logs, CompileChat, Erreur)

### ğŸ”Œ Proxy Multi-Provider
- Redirection transparente vers **8 providers** : Kimi Code, NVIDIA, Mistral, OpenRouter, SiliconFlow, Groq, Cerebras, Gemini
- Streaming SSE et requÃªtes non-streaming supportÃ©es
- SÃ©lection granulaire du modÃ¨le dans l'UI (20+ modÃ¨les disponibles)
- Interception automatique des requÃªtes `chat/completions`
- Injection robuste des clÃ©s API depuis `config.toml`
- Mise Ã  jour automatique du header `Host` pour Ã©viter les erreurs 401
- Conservation de la latence temps rÃ©el

## L'astuce qui change tout : Le Sanitizer

J'ai remarquÃ© que 30-40% de mes tokens Ã©taient gaspillÃ©s dans des messages tools/console que je ne lisais jamais. C'est comme avoir un restaurant oÃ¹ le serveur vous lit Ã  voix haute les tickets de cuisine - inutile et verbeux.

### âŒ L'approche naÃ¯ve
```python
# Envoyer tout Ã  l'API, y compris le bruit
messages = [
    {"role": "system", "content": "You are helpful"},
    {"role": "user", "content": "Help me debug"},
    {"role": "assistant", "content": "I'll help"},
    {"role": "tool", "content": "[500+ tokens de logs systÃ¨me]"}  # GaspillÃ©!
]
```

### âœ… L'approche intelligente
```python
# Masquer ce qui n'apporte pas de valeur
messages = [
    {"role": "system", "content": "You are helpful"},
    {"role": "user", "content": "Help me debug"},
    {"role": "assistant", "content": "I'll help"},
    # Message tool masquÃ© automatiquement, rÃ©cupÃ©rable si besoin
]
```

Le sanitizer dÃ©tecte automatiquement les messages > 1000 tokens de type tool/console, les masque pendant l'envoi, et les stocke pour rÃ©cupÃ©ration ultÃ©rieure. RÃ©sultat : 20-40% d'Ã©conomie de tokens sans perte d'information.

## Les trois phases de gestion du contexte

Pensez Ã  votre conversation comme Ã  un restaurant. Le sanitizer, c'est le serveur qui ne vous lit pas les tickets de cuisine. MCP, c'est votre sommelier qui se souvient de vos prÃ©fÃ©rences. La compression, c'est quand vous demandez l'addition et le serveur vous rÃ©sume ce que vous avez commandÃ©.

### Phase 1 : Sanitizer - Le tri automatique
Le systÃ¨me masque les messages tools/console verbeux (>1000 tokens) et les remplace par une note "contenu masquÃ©". C'est rÃ©cupÃ©rable via `/api/mask/{hash}` si vous en avez besoin.

### Phase 2 : MCP - La mÃ©moire Ã  long terme  
DÃ©tecte les balises `<mcp-memory>`, `@memory[]`, `@recall()` pour distinguer ce qui est stockÃ© Ã  long terme vs ce qui fait partie de la conversation actuelle. Indicateur violet/rose sur le dashboard.

### Phase 3 : MCP & Compression - L'intelligence augmentÃ©e
Quand vous dÃ©passez 85% du contexte, un bouton apparaÃ®t. Mais ce n'est pas tout :

**Compression Intelligente** : PrÃ©serve les messages systÃ¨me, garde les 5 derniers Ã©changes, rÃ©sume le reste avec le LLM actuel.

**MCP AvancÃ©** : IntÃ©gration de serveurs MCP externes pour des optimisations de niveau supÃ©rieur :
- **Qdrant MCP** : Recherche sÃ©mantique en <50ms pour trouver des patterns similaires dans votre historique
- **Context Compression MCP** : Compression avancÃ©e 20-80% avec stockage persistant
- **Routage Intelligent** : Le systÃ¨me choisit automatiquement le provider avec le meilleur ratio capacitÃ©/coÃ»t/latence
- **MÃ©moire StandardisÃ©e** : Distinction entre mÃ©moires frÃ©quentes (patterns), Ã©pisodiques (conversations), et sÃ©mantiques (vecteurs)

### Phase 4 : Nouveaux Serveurs MCP - L'Ã©cosystÃ¨me Ã©tendu
Quatre serveurs MCP supplÃ©mentaires pour Ã©tendre les capacitÃ©s du proxy :

**Shrimp Task Manager MCP** (14 outils) : Gestion de tÃ¢ches complÃ¨te avec priorisation, dÃ©pendances et analyse de complexitÃ©. IntÃ¨gre `get_tasks`, `parse_prd`, `expand_task`, `analyze_project_complexity` et plus.

**Sequential Thinking MCP** (1 outil) : Raisonnement sÃ©quentiel structurÃ© pour rÃ©soudre des problÃ¨mes complexes Ã©tape par Ã©tape, avec support de branches et rÃ©visions.

**Fast Filesystem MCP** (25 outils) : OpÃ©rations fichiers haute performance - lecture, Ã©criture, recherche de code, Ã©dition block-safe, compression, synchronisation de rÃ©pertoires.

**JSON Query MCP** (3 outils) : RequÃªtes JSON avancÃ©es avec JSONPath, recherche de clÃ©s et valeurs dans de gros fichiers JSON.

## Le Dashboard en temps rÃ©el

C'est comme avoir un tableau de bord de voiture pour votre conversation LLM. La jauge change de couleur au fur et Ã  mesure que vous consommez votre contexte, et vous voyez exactement d'oÃ¹ viennent les tokens.

### La magie du Log Watcher
Continue.dev Ã©crit tout dans `~/.continue/logs/core.log`. Mon systÃ¨me surveille ce fichier en temps rÃ©el, extrait les mÃ©triques, et les fusionne intelligemment avec les donnÃ©es du proxy.

**PrioritÃ© de fusion** : CompileChat (violet) > Erreur (rouge) > Proxy (bleu) > Logs (vert)

Le rÃ©sultat? Une vue unifiÃ©e et prÃ©cise de votre consommation, mÃªme si vous utilisez plusieurs sources de donnÃ©es.

### Ce que vous voyez
- **Jauge de contexte** : Vert (sÃ»r) â†’ Jaune (attention) â†’ Rouge (urgent)
- **Graphique historique** : Ã‰volution des tokens dans le temps
- **Source des tokens** : Couleur selon l'origine
- **Export instantanÃ©** : CSV ou JSON pour analyse

## Comment Ã§a marche : L'architecture modulaire

J'ai commencÃ© avec un fichier monolithique de 3,073 lignes. C'Ã©tait comme avoir toute une maison dans une seule piÃ¨ce - impossible Ã  entretenir. Maintenant, c'est organisÃ© par Ã©tages :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           API Layer (FastAPI)          â”‚  â† Interface utilisateur
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Services Layer                 â”‚  â† WebSocket, Rate Limiting  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Features Layer                 â”‚  â† Sanitizer, MCP, Compression
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Proxy Layer                    â”‚  â† Routage vers les APIs
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Core Layer                     â”‚  â† Database, Tokens, Models
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pourquoi cette structure?** Chaque couche ne dÃ©pend que de celles en dessous. Je peux tester le systÃ¨me de tokens sans dÃ©marrer l'API. Je peux remplacer le sanitizer sans casser le proxy. C'est maintenabilitÃ© et Ã©volutivitÃ©.

## Installation rapide

Vous avez 5 minutes? C'est tout ce qu'il vous faut.

```bash
# 2. Configurer vos clÃ©s API (mÃ©thode recommandÃ©e)
# Copier le template et Ã©diter avec vos vraies clÃ©s
cp .env.example .env
# Ã‰diter .env avec vos clÃ©s API (voir section ci-dessous)

# 3. DÃ©marrer (chargement automatique des variables d'environnement)
./scripts/start.sh
# Dashboard sur http://localhost:8000
```

### Configuration des variables d'environnement

Au lieu d'Ã©diter manuellement `config.toml`, vous pouvez utiliser le fichier `.env` pour une configuration plus simple et sÃ©curisÃ©e :

```bash
# Copier le template
cp .env.example .env

# Ã‰diter avec vos vraies clÃ©s API
nano .env  # ou votre Ã©diteur prÃ©fÃ©rÃ©
```

**Variables requises :**
- `KIMI_API_KEY` : ClÃ© principale Kimi Code
- `NVIDIA_API_KEY` : Pour les modÃ¨les Kimi sur NVIDIA
- `MISTRAL_API_KEY` : Pour les modÃ¨les Mistral AI

**Variables optionnelles (tiers gratuits disponibles) :**
- `OPENROUTER_API_KEY` : AccÃ¨s multi-provider
- `SILICONFLOW_API_KEY` : CrÃ©dits gratuits disponibles
- `GROQ_API_KEY` : InfÃ©rence ultra-rapide, tier gratuit
- `CEREBRAS_API_KEY` : Tier gratuit disponible
- `GEMINI_API_KEY` : Tier gratuit disponible

**Utilisation automatique :**
Le script `./scripts/start.sh` charge automatiquement le fichier `.env` s'il existe. Si le fichier n'existe pas, il affiche des instructions pour le crÃ©er.

Le fichier `.env` est automatiquement ignorÃ© par Git pour Ã©viter de committer vos clÃ©s API.

### Configuration Continue (optionnel)
```bash
cp config.yaml ~/.continue/config.yaml
```
Continue.dev utilisera automatiquement les modÃ¨les configurÃ©s.

## Les modÃ¨les que j'utilise

J'ai testÃ© 20+ modÃ¨les pour trouver le meilleur Ã©quilibre coÃ»t/performance. Voici mes prÃ©fÃ©rÃ©s :

| ModÃ¨le | Provider | Pourquoi je l'aime | Contexte |
|--------|----------|-------------------|----------|
| `kimi-code/kimi-for-coding` | ğŸŒ™ Kimi Code | **Le meilleur pour le code** - thinking intÃ©grÃ© | 256K |
| `nvidia/kimi-k2.5` | ğŸŸ¢ NVIDIA | **Rapide et pas cher** - $0.001/1K tokens | 256K |
| `mistral/codestral-2501` | ğŸ”· Mistral | **SpÃ©cialisÃ© coding** - autocomplete incroyable | 256K |
| `openrouter/aurora-alpha` | ğŸ”€ OpenRouter | **Ã‰quilibre parfait** - bon marchÃ© et capable | 128K |
| `gemini/gemini-2.5-pro` | ğŸ’ Gemini | **Le plus puissant** - multimodal et 1M context | 1M |

**Mon choix quotidien** : Kimi Code pour le dÃ©veloppement sÃ©rieux, NVIDIA K2.5 pour les tests rapides.

### Le truc des providers
Chaque provider a ses particularitÃ©s. Kimi Code est cher mais incroyablement intelligent. NVIDIA est ultra-rapide mais limitÃ© Ã  3 modÃ¨les. OpenRouter me donne accÃ¨s Ã  tout mais avec une latence supplÃ©mentaire.

Le systÃ¨me gÃ¨re tout Ã§a automatiquement - vous choisissez juste dans l'interface.

## Au quotidien : Comment je l'utilise

### Le Dashboard
Ouvrez `http://localhost:8000`. C'est lÃ  que je passe ma journÃ©e :

- **Jauge de contexte** : Je vois quand j'approche des limites (vert â†’ jaune â†’ rouge)
- **Logs en temps rÃ©el** : Chaque requÃªte apparaÃ®t instantanÃ©ment avec sa source
- **Nouvelle session** : Je change de provider/modÃ¨le en un clic
- **Export** : Je tÃ©lÃ©charge CSV pour analyser mes coÃ»ts mensuels

### La CLI que j'adore
```bash
./bin/kimi-proxy start      # DÃ©marrer
./bin/kimi-proxy status     # "Running on port 8000, 3 active sessions"
./bin/kimi-proxy logs       # Voir les derniÃ¨res requÃªtes
./bin/kimi-proxy stop       # ArrÃªt propre
```

### L'intÃ©gration Continue.dev
Une fois configurÃ©, Continue.dev envoie tout Ã  travers le proxy sans que j'y pense. Les tokens apparaissent magiquement sur le dashboard.

### Quand j'utilise la compression
Le bouton rouge apparaÃ®t Ã  85% du contexte. Un clic, et le systÃ¨me :
1. PrÃ©serve mes instructions systÃ¨me
2. Garde les 5 derniers Ã©changes  
3. RÃ©sume tout le reste intelligemment
4. Me fait Ã©conomiser 60% de tokens

C'est le bouton "oh non je vais perdre mon contexte" qui me sauve la vie.

## La RÃ¨gle d'Or : Transparence totale

**Le principe** : Chaque token comptÃ© doit Ãªtre visible, expliquÃ© et optimisable.

Je voulais savoir exactement ce que je paie. Pas de "coÃ»ts estimÃ©s", pas de "tokens approximatifs". Tiktoken compte prÃ©cisÃ©ment chaque entrÃ©e/sortie. Le dashboard me montre la source exacte. Le sanitizer me fait Ã©conomiser ce qui est gaspillÃ©.

## DÃ©pannage rapide

### Erreur 401?
VÃ©rifiez votre `api_key` dans `config.toml`. Le proxy met automatiquement Ã  jour le header `Host` - c'est une erreur classique que j'ai dÃ©jÃ  rÃ©solue.

### Log Watcher ne dÃ©tecte rien?
Continue doit Ã©crire dans `~/.continue/logs/core.log`. VÃ©rifiez `/health` pour voir si le fichier existe.

### Port dÃ©jÃ  utilisÃ©?
```bash
./bin/kimi-proxy stop && ./bin/kimi-proxy start
```

### Base de donnÃ©es corrompue?
```bash
./scripts/backup.sh  # Backup d'abord!
rm sessions.db && ./bin/kimi-proxy start
```

## Pourquoi je partage Ã§a

J'ai passÃ© des mois Ã  optimiser ma consommation LLM. Au dÃ©but, c'Ã©tait juste un script perso. Puis mes collÃ¨gues l'ont utilisÃ©. Puis j'ai ajoutÃ© le sanitizer, la compression, l'architecture modulaire...

Aujourd'hui, c'est un systÃ¨me complet qui me fait Ã©conomiser des centaines d'euros chaque mois. Si Ã§a peut aider d'autres dÃ©veloppeurs Ã  comprendre et contrÃ´ler leur consommation, tant mieux.

---

**Note technique** : Le projet est optimisÃ© pour PyCharm + Continue.dev, mais fonctionne parfaitement avec VS Code ou n'importe quel client compatible OpenAI.
