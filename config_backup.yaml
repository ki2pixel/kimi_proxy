name: Kimi Proxy Dashboard Configuration
version: 3.0.0
schema: v1

# ============================================
# CONFIGURATION PROXY - Tous les modÃ¨les
# ============================================

models:
  # === NVIDIA ===
  - name: ðŸ”¥ NVIDIA - Kimi K2.5 (256K)
    provider: openai
    model: nvidia/kimi-k2.5
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 262144
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
      - thinking
      - video_in
      - image_in
    defaultCompletionOptions:
      temperature: 0.6
      maxTokens: 32768
      contextLength: 262144

  - name: ðŸ”¥ NVIDIA - Kimi K2 Thinking (256K)
    provider: openai
    model: nvidia/kimi-k2-thinking
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 262144
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
      - thinking
      - video_in
      - image_in
    defaultCompletionOptions:
      temperature: 1.0
      maxTokens: 32768
      contextLength: 262144

  # === NVIDIA - Mistral Large 3 ===
  - name: ðŸ”¥ NVIDIA - Mistral Large 3 - 675b (262K)
    provider: openai
    model: nvidia/mistral-large-3-675b
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 262144
    roles:
      - chat
      - edit
      - apply
    capabilities:
      - tool_use
    defaultCompletionOptions:
      temperature: 0.2
      maxTokens: 8192
      contextLength: 262144

  # === NVIDIA - GPT-OSS 120B ===
  - name: ðŸ”¥ NVIDIA - GPT-OSS 120B (131K)
    provider: openai
    model: nvidia/gpt-oss-120b
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 131072
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    defaultCompletionOptions:
      temperature: 0.2
      maxTokens: 8192
      contextLength: 131072

  # === NVIDIA - DeepSeek V3.2 ===
  - name: ðŸ”¥ NVIDIA - DeepSeek V3.2 (164K)
    provider: openai
    model: nvidia/deepseek-v3.2
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 164000
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
      - thinking
    defaultCompletionOptions:
      temperature: 0.4
      maxTokens: 8192
      contextLength: 164000
      chat_template_kwargs:
        thinking: true

  # === NVIDIA - GLM-4.7 ===
  - name: NVIDIA - GLM-4.7 (64K)
    provider: openai
    model: nvidia/glm-4.7
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 64000
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    autocompleteOptions:
      debounceDelay: 200
      maxPromptTokens: 1024
      onlyMyCode: true
    capabilities:
      - thinking
    defaultCompletionOptions:
      temperature: 0.2
      maxTokens: 8192
      contextLength: 64000
      chat_template_kwargs:
        enable_thinking: true
        clear_thinking: false

  # === NVIDIA - GLM-5 ===
  - name: NVIDIA - GLM-5 (204K)
    provider: openai
    model: nvidia/glm-5
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 204800
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
      - thinking
    defaultCompletionOptions:
      temperature: 0.2
      maxTokens: 8192
      contextLength: 204800
      chat_template_kwargs:
        enable_thinking: true
        clear_thinking: false

  # === NVIDIA - Qwen3 Coder 480B ===
  - name: NVIDIA - Qwen3 Coder 480B (262K)
    provider: openai
    model: nvidia/qwen3-coder-480b
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 262144
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
      - coding
    defaultCompletionOptions:
      temperature: 0.2
      maxTokens: 8192
      contextLength: 262144

  # === NVIDIA - Qwen3 Next 80B Thinking ===
  - name: NVIDIA - Qwen3 Next 80B Thinking (128K)
    provider: openai
    model: nvidia/qwen3-next-80b-a3b-thinking
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 131072
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
      - thinking
    defaultCompletionOptions:
      temperature: 0.2
      maxTokens: 8192
      contextLength: 131072

  # === OpenRouter ===
  - name: OpenRouter - Aurora Alpha (128K)
    provider: openai
    model: openrouter/aurora-alpha
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 128000
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      temperature: 0.2
      maxTokens: 8192
      contextLength: 128000

  # === Gemini ===
  - name: Gemini - 2.5 Flash Lite (1M)
    provider: openai
    model: gemini/gemini-2.5-flash-lite
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 1048576
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      temperature: 0.3
      maxTokens: 8192
      contextLength: 1048576

  - name: Gemini - 3 Flash Preview (1M)
    provider: openai
    model: gemini/gemini-3-flash-preview
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 1048576
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      temperature: 0.3
      maxTokens: 8192
      contextLength: 1048576

  - name: Gemini - 2.5 Flash (1M)
    provider: openai
    model: gemini/gemini-2.5-flash
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 1048576
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      temperature: 0.3
      maxTokens: 8192
      contextLength: 1048576

  - name: Gemini - 2.5 Pro (1M)
    provider: openai
    model: gemini/gemini-2.5-pro
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 1048576
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      temperature: 0.3
      maxTokens: 8192
      contextLength: 1048576

  # === NanoGPT - Moonshot AI ===
  - name: ðŸŒ™ NanoGPT - Moonshot AI Kimi K2.5 (256K)
    provider: openai
    model: nano-gpt/kimi-k2.5
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 262144
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
      - thinking
      - video_in
      - image_in
    defaultCompletionOptions:
      temperature: 0.6
      maxTokens: 32768
      contextLength: 262144

  - name: ðŸŒ™ NanoGPT - Moonshot AI Kimi K2.5 Thinking (256K)
    provider: openai
    model: nano-gpt/kimi-k2.5-thinking
    apiBase: http://127.0.0.1:8000
    apiKey: dummy-key
    contextLength: 262144
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
      - thinking
      - video_in
      - image_in
    defaultCompletionOptions:
      temperature: 1.0
      maxTokens: 32768
      contextLength: 262144

# ============================================
# MCP SERVERS - SÃ‰LECTIFS
# Seuls les serveurs pour context providers Continue gardÃ©s
# Les MCP tools avancÃ©s sont via le proxy Kimi
# ============================================
mcpServers:
  # Context providers Continue (nÃ©cessaires)
  - name: filesystem-agent
    command: npx
    args: ["-y", "@modelcontextprotocol/server-filesystem", "/home/kidpixel"]
    env:
      PATH: "/usr/bin:/bin:/usr/local/bin"

  - name: ripgrep-agent
    command: npx
    args: ["-y", "mcp-ripgrep"]
    env:
      PATH: "/usr/bin:/bin:/usr/local/bin"

  # MCP Phase 4 - Processus locaux (solution finale)
  - name: shrimp-task-manager
    command: /home/kidpixel/.local/bin/shrimp-task-manager
    args: [ ]

  - name: sequential-thinking
    command: npx
    args: ["-y", "mcp-sequentialthinking-tools"]
    env:
      MAX_HISTORY_SIZE: "1000"
      PATH: "/usr/bin:/bin:/usr/local/bin"

  - name: fast-filesystem
    command: npx
    args: ["-y", "fast-filesystem-mcp"]
    env:
      CREATE_BACKUP_FILES: "false"
      PATH: "/usr/bin:/bin:/usr/local/bin"

  - name: json-query
    command: node
    args: [ "/home/kidpixel/kimi-proxy/mcp/json-query-mcp-main/dist/index.js" ]

  # Services externes Continue (nÃ©cessaires)
  - name: render-signal-mcp
    url: "https://mcp.render.com/mcp"
    headers:
      Authorization: "Bearer rnd_VQWUOIKNaGHBXm1S71xdm06nit49"

  - name: render-photomaton-mcp
    url: "https://mcp.render.com/mcp"
    headers:
      Authorization: "Bearer rnd_Fp6xGSjGLi59nimXWhWek4m5012a"

  - name: render-switchbot-mcp
    url: "https://mcp.render.com/mcp"
    headers:
      Authorization: "Bearer rnd_VKbtKTHo27mUYxbI99z6ZqBY3v4J"

  - name: redis-signal-mcp-server
    command: uvx
    args:
      - "--from"
      - "redis-mcp-server@latest"
      - "redis-mcp-server"
      - "--url"
      - "redis://red-d0l2c17fte5s73955i40:KCL5iVxykagTchnJxhccAyCp9S8dWFls@red-d0l2c17fte5s73955i40:6379"

  - name: photomaton-postgres
    command: uvx
    args: [ "--python", "3.12", "postgres-mcp", "--access-mode=unrestricted" ]
    env:
      DATABASE_URI: "postgresql://neondb_owner:npg_GwWL5Fltq2kO@ep-shiny-star-ad5t7thb-pooler.c-2.us-east-1.aws.neon.tech/neondb?sslmode=require&channel_binding=require"
    connectionTimeout: 60000

  - name: switchbot-postgres
    command: uvx
    args: [ "--python", "3.12", "postgres-mcp", "--access-mode=unrestricted" ]
    env:
      DATABASE_URI: "postgresql://postgres.tpjbanbcvfwyhoqsuyxf:%40vH2f85CAXjm96tz@aws-1-eu-central-1.pooler.supabase.com:5432/postgres?sslmode=require"
    connectionTimeout: 60000