# üöÄ Kimi Proxy Dashboard

**TL;DR**: Je construis un proxy transparent qui me fait √©conomiser 20-40% de tokens LLM tout en me montrant exactement ce que je consomme en temps r√©el.

J'√©tais fatigu√© de payer $30/mois pour des services de transcription alors que les APIs me co√ªtaient $0.36/heure. Avec Continue.dev dans PyCharm, je voyais bien mes tokens s'envoler, mais je ne savais pas combien ni pourquoi. Pire encore, je devais choisir entre perdre mon contexte ou payer encore plus.

Alors j'ai construit Kimi Proxy Dashboard. C'est comme avoir un compteur intelligent entre mon IDE et les APIs LLM; il intercepte tout, compte pr√©cis√©ment les tokens avec Tiktoken, et me montre exactement o√π je d√©pense. Le meilleur? Il compresse automatiquement les conversations verbeuses pour me faire √©conomiser jusqu'√† 40% de tokens sans que je perde l'essentiel.

![Dashboard](https://img.shields.io/badge/Dashboard-Live-success?style=for-the-badge)
![FastAPI](https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![SQLite](https://img.shields.io/badge/SQLite-07405E?style=for-the-badge&logo=sqlite&logoColor=white)
![WebSocket](https://img.shields.io/badge/WebSocket-Realtime-blue?style=for-the-badge)
![Log Watcher](https://img.shields.io/badge/Log%20Watcher-PyCharm-purple?style=for-the-badge)
![Architecture](https://img.shields.io/badge/Architecture-Modulaire-orange?style=for-the-badge)

## Ce que √ßa r√©sout vraiment

### ‚ùå Avant : La bo√Æte noire des tokens
- Pas de visibilit√© sur ma consommation r√©elle
- Contexte qui dispara√Æt au milieu d'une conversation importante  
- Factures API qui montent sans comprendre pourquoi
- Perte de productivit√© √† devoir recr√©er le contexte

### ‚úÖ Apr√®s : Le contr√¥le total
- **Jauge visuelle instantan√©e** : Je vois exactement o√π j'en suis (Vert ‚Üí Jaune ‚Üí Rouge)
- **√âconomie automatique** : Le sanitizer masque les messages tools/console verbeux
- **Compression intelligente** : Un bouton d'urgence quand je d√©passe 85% du contexte
- **Historique complet** : Chaque token compt√© avec sa source (Proxy, Logs, CompileChat, Erreur)

### üîå Proxy Multi-Provider
- Redirection transparente vers **8 providers** : Kimi Code, NVIDIA, Mistral, OpenRouter, SiliconFlow, Groq, Cerebras, Gemini
- Streaming SSE et requ√™tes non-streaming support√©es
- S√©lection granulaire du mod√®le dans l'UI (20+ mod√®les disponibles)
- Interception automatique des requ√™tes `chat/completions`
- Injection robuste des cl√©s API depuis `config.toml`
- Mise √† jour automatique du header `Host` pour √©viter les erreurs 401
- Conservation de la latence temps r√©el

## L'astuce qui change tout : Le Sanitizer

J'ai remarqu√© que 30-40% de mes tokens √©taient gaspill√©s dans des messages tools/console que je ne lisais jamais. C'est comme avoir un restaurant o√π le serveur vous lit √† voix haute les tickets de cuisine - inutile et verbeux.

### ‚ùå L'approche na√Øve
```python
# Envoyer tout √† l'API, y compris le bruit
messages = [
    {"role": "system", "content": "You are helpful"},
    {"role": "user", "content": "Help me debug"},
    {"role": "assistant", "content": "I'll help"},
    {"role": "tool", "content": "[500+ tokens de logs syst√®me]"}  # Gaspill√©!
]
```

### ‚úÖ L'approche intelligente
```python
# Masquer ce qui n'apporte pas de valeur
messages = [
    {"role": "system", "content": "You are helpful"},
    {"role": "user", "content": "Help me debug"},
    {"role": "assistant", "content": "I'll help"},
    # Message tool masqu√© automatiquement, r√©cup√©rable si besoin
]
```

Le sanitizer d√©tecte automatiquement les messages > 1000 tokens de type tool/console, les masque pendant l'envoi, et les stocke pour r√©cup√©ration ult√©rieure. R√©sultat : 20-40% d'√©conomie de tokens sans perte d'information.

## Les trois phases de gestion du contexte

Pensez √† votre conversation comme √† un restaurant. Le sanitizer, c'est le serveur qui ne vous lit pas les tickets de cuisine. MCP, c'est votre sommelier qui se souvient de vos pr√©f√©rences. La compression, c'est quand vous demandez l'addition et le serveur vous r√©sume ce que vous avez command√©.

### Phase 1 : Sanitizer - Le tri automatique
Le syst√®me masque les messages tools/console verbeux (>1000 tokens) et les remplace par une note "contenu masqu√©". C'est r√©cup√©rable via `/api/mask/{hash}` si vous en avez besoin.

### Phase 2 : MCP - La m√©moire √† long terme  
D√©tecte les balises `<mcp-memory>`, `@memory[]`, `@recall()` pour distinguer ce qui est stock√© √† long terme vs ce qui fait partie de la conversation actuelle. Indicateur violet/rose sur le dashboard.

### Phase 3 : MCP & Compression - L'intelligence augment√©e
Quand vous d√©passez 85% du contexte, un bouton appara√Æt. Mais ce n'est pas tout :

**Compression Intelligente** : Pr√©serve les messages syst√®me, garde les 5 derniers √©changes, r√©sume le reste avec le LLM actuel.

**MCP Avanc√©** : Int√©gration de serveurs MCP externes pour des optimisations de niveau sup√©rieur :
- **Qdrant MCP** : Recherche s√©mantique en <50ms pour trouver des patterns similaires dans votre historique
- **Context Compression MCP** : Compression avanc√©e 20-80% avec stockage persistant
- **Routage Intelligent** : Le syst√®me choisit automatiquement le provider avec le meilleur ratio capacit√©/co√ªt/latence
- **M√©moire Standardis√©e** : Distinction entre m√©moires fr√©quentes (patterns), √©pisodiques (conversations), et s√©mantiques (vecteurs)

### Phase 4 : Nouveaux Serveurs MCP - L'√©cosyst√®me √©tendu
Quatre serveurs MCP suppl√©mentaires pour √©tendre les capacit√©s du proxy :

**Task Master MCP** (14 outils) : Gestion de t√¢ches compl√®te avec priorisation, d√©pendances et analyse de complexit√©. Int√®gre `get_tasks`, `parse_prd`, `expand_task`, `analyze_project_complexity` et plus.

**Sequential Thinking MCP** (1 outil) : Raisonnement s√©quentiel structur√© pour r√©soudre des probl√®mes complexes √©tape par √©tape, avec support de branches et r√©visions.

**Fast Filesystem MCP** (25 outils) : Op√©rations fichiers haute performance - lecture, √©criture, recherche de code, √©dition block-safe, compression, synchronisation de r√©pertoires.

**JSON Query MCP** (3 outils) : Requ√™tes JSON avanc√©es avec JSONPath, recherche de cl√©s et valeurs dans de gros fichiers JSON.

## Le Dashboard en temps r√©el

C'est comme avoir un tableau de bord de voiture pour votre conversation LLM. La jauge change de couleur au fur et √† mesure que vous consommez votre contexte, et vous voyez exactement d'o√π viennent les tokens.

### La magie du Log Watcher
Continue.dev √©crit tout dans `~/.continue/logs/core.log`. Mon syst√®me surveille ce fichier en temps r√©el, extrait les m√©triques, et les fusionne intelligemment avec les donn√©es du proxy.

**Priorit√© de fusion** : CompileChat (violet) > Erreur (rouge) > Proxy (bleu) > Logs (vert)

Le r√©sultat? Une vue unifi√©e et pr√©cise de votre consommation, m√™me si vous utilisez plusieurs sources de donn√©es.

### Ce que vous voyez
- **Jauge de contexte** : Vert (s√ªr) ‚Üí Jaune (attention) ‚Üí Rouge (urgent)
- **Graphique historique** : √âvolution des tokens dans le temps
- **Source des tokens** : Couleur selon l'origine
- **Export instantan√©** : CSV ou JSON pour analyse

## Comment √ßa marche : L'architecture modulaire

J'ai commenc√© avec un fichier monolithique de 3,073 lignes. C'√©tait comme avoir toute une maison dans une seule pi√®ce - impossible √† entretenir. Maintenant, c'est organis√© par √©tages :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           API Layer (FastAPI)          ‚îÇ  ‚Üê Interface utilisateur
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Services Layer                 ‚îÇ  ‚Üê WebSocket, Rate Limiting  
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Features Layer                 ‚îÇ  ‚Üê Sanitizer, MCP, Compression
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Proxy Layer                    ‚îÇ  ‚Üê Routage vers les APIs
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         Core Layer                     ‚îÇ  ‚Üê Database, Tokens, Models
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Pourquoi cette structure?** Chaque couche ne d√©pend que de celles en dessous. Je peux tester le syst√®me de tokens sans d√©marrer l'API. Je peux remplacer le sanitizer sans casser le proxy. C'est maintenabilit√© et √©volutivit√©.

## Installation rapide

Vous avez 5 minutes? C'est tout ce qu'il vous faut.

```bash
# 2. Configurer vos cl√©s API (m√©thode recommand√©e)
# Copier le template et √©diter avec vos vraies cl√©s
cp .env.example .env
# √âditer .env avec vos cl√©s API (voir section ci-dessous)

# 3. Charger automatiquement les variables d'environnement
source ./load-env.sh

# 4. D√©marrer
./bin/kimi-proxy start
# Dashboard sur http://localhost:8000
```

### Configuration des variables d'environnement

Au lieu d'√©diter manuellement `config.toml`, vous pouvez utiliser le fichier `.env` pour une configuration plus simple et s√©curis√©e :

```bash
# Copier le template
cp .env.example .env

# √âditer avec vos vraies cl√©s API
nano .env  # ou votre √©diteur pr√©f√©r√©
```

**Variables requises :**
- `KIMI_API_KEY` : Cl√© principale Kimi Code
- `NVIDIA_API_KEY` : Pour les mod√®les Kimi sur NVIDIA
- `MISTRAL_API_KEY` : Pour les mod√®les Mistral AI

**Variables optionnelles (tiers gratuits disponibles) :**
- `OPENROUTER_API_KEY` : Acc√®s multi-provider
- `SILICONFLOW_API_KEY` : Cr√©dits gratuits disponibles
- `GROQ_API_KEY` : Inf√©rence ultra-rapide, tier gratuit
- `CEREBRAS_API_KEY` : Tier gratuit disponible
- `GEMINI_API_KEY` : Tier gratuit disponible

**Utilisation automatique :**
```bash
# Charger les variables √† chaque session
source ./load-env.sh

# Ou ajouter √† votre ~/.bashrc pour chargement automatique
echo "source $(pwd)/load-env.sh" >> ~/.bashrc
```

Le fichier `.env` est automatiquement ignor√© par Git pour √©viter de committer vos cl√©s API.

### Configuration Continue (optionnel)
```bash
cp config.yaml ~/.continue/config.yaml
```
Continue.dev utilisera automatiquement les mod√®les configur√©s.

## Les mod√®les que j'utilise

J'ai test√© 20+ mod√®les pour trouver le meilleur √©quilibre co√ªt/performance. Voici mes pr√©f√©r√©s :

| Mod√®le | Provider | Pourquoi je l'aime | Contexte |
|--------|----------|-------------------|----------|
| `kimi-code/kimi-for-coding` | üåô Kimi Code | **Le meilleur pour le code** - thinking int√©gr√© | 256K |
| `nvidia/kimi-k2.5` | üü¢ NVIDIA | **Rapide et pas cher** - $0.001/1K tokens | 256K |
| `mistral/codestral-2501` | üî∑ Mistral | **Sp√©cialis√© coding** - autocomplete incroyable | 256K |
| `openrouter/aurora-alpha` | üîÄ OpenRouter | **√âquilibre parfait** - bon march√© et capable | 128K |
| `gemini/gemini-2.5-pro` | üíé Gemini | **Le plus puissant** - multimodal et 1M context | 1M |

**Mon choix quotidien** : Kimi Code pour le d√©veloppement s√©rieux, NVIDIA K2.5 pour les tests rapides.

### Le truc des providers
Chaque provider a ses particularit√©s. Kimi Code est cher mais incroyablement intelligent. NVIDIA est ultra-rapide mais limit√© √† 3 mod√®les. OpenRouter me donne acc√®s √† tout mais avec une latence suppl√©mentaire.

Le syst√®me g√®re tout √ßa automatiquement - vous choisissez juste dans l'interface.

## Au quotidien : Comment je l'utilise

### Le Dashboard
Ouvrez `http://localhost:8000`. C'est l√† que je passe ma journ√©e :

- **Jauge de contexte** : Je vois quand j'approche des limites (vert ‚Üí jaune ‚Üí rouge)
- **Logs en temps r√©el** : Chaque requ√™te appara√Æt instantan√©ment avec sa source
- **Nouvelle session** : Je change de provider/mod√®le en un clic
- **Export** : Je t√©l√©charge CSV pour analyser mes co√ªts mensuels

### La CLI que j'adore
```bash
./bin/kimi-proxy start      # D√©marrer
./bin/kimi-proxy status     # "Running on port 8000, 3 active sessions"
./bin/kimi-proxy logs       # Voir les derni√®res requ√™tes
./bin/kimi-proxy stop       # Arr√™t propre
```

### L'int√©gration Continue.dev
Une fois configur√©, Continue.dev envoie tout √† travers le proxy sans que j'y pense. Les tokens apparaissent magiquement sur le dashboard.

### Quand j'utilise la compression
Le bouton rouge appara√Æt √† 85% du contexte. Un clic, et le syst√®me :
1. Pr√©serve mes instructions syst√®me
2. Garde les 5 derniers √©changes  
3. R√©sume tout le reste intelligemment
4. Me fait √©conomiser 60% de tokens

C'est le bouton "oh non je vais perdre mon contexte" qui me sauve la vie.

## La R√®gle d'Or : Transparence totale

**Le principe** : Chaque token compt√© doit √™tre visible, expliqu√© et optimisable.

Je voulais savoir exactement ce que je paie. Pas de "co√ªts estim√©s", pas de "tokens approximatifs". Tiktoken compte pr√©cis√©ment chaque entr√©e/sortie. Le dashboard me montre la source exacte. Le sanitizer me fait √©conomiser ce qui est gaspill√©.

## D√©pannage rapide

### Erreur 401?
V√©rifiez votre `api_key` dans `config.toml`. Le proxy met automatiquement √† jour le header `Host` - c'est une erreur classique que j'ai d√©j√† r√©solue.

### Log Watcher ne d√©tecte rien?
Continue doit √©crire dans `~/.continue/logs/core.log`. V√©rifiez `/health` pour voir si le fichier existe.

### Port d√©j√† utilis√©?
```bash
./bin/kimi-proxy stop && ./bin/kimi-proxy start
```

### Base de donn√©es corrompue?
```bash
./scripts/backup.sh  # Backup d'abord!
rm sessions.db && ./bin/kimi-proxy start
```

## Pourquoi je partage √ßa

J'ai pass√© des mois √† optimiser ma consommation LLM. Au d√©but, c'√©tait juste un script perso. Puis mes coll√®gues l'ont utilis√©. Puis j'ai ajout√© le sanitizer, la compression, l'architecture modulaire...

Aujourd'hui, c'est un syst√®me complet qui me fait √©conomiser des centaines d'euros chaque mois. Si √ßa peut aider d'autres d√©veloppeurs √† comprendre et contr√¥ler leur consommation, tant mieux.

---

**Note technique** : Le projet est optimis√© pour PyCharm + Continue.dev, mais fonctionne parfaitement avec VS Code ou n'importe quel client compatible OpenAI.
