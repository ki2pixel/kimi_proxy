name: Continue Configuration
version: 2.0.0
schema: v1

models:
  # === MOD√àLES MISTRAL (API native - toujours disponibles) ===
  # Mod√®le principal pour le codage
  - name: Mistral - Codestral 2501 (Codage) - (256K)
    provider: mistral  # Use openai to bypass hardcoded limits
    model: codestral-2501
    apiBase: https://api.mistral.ai/v1
    apiKey: ${MISTRAL_API_KEY}
    contextLength: 262144
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    autocompleteOptions:
      debounceDelay: 200
      maxPromptTokens: 1024
      onlyMyCode: true
    defaultCompletionOptions:
      temperature: 0.1
      maxTokens: 8192

  - name: üî• Mistral - Mistral Large 2411 (Principal) - (128K)
    provider: mistral  # Use openai to bypass hardcoded limits
    model: mistral-large-2411
    apiBase: https://api.mistral.ai/v1
    apiKey: ${MISTRAL_API_KEY}
    contextLength: 131072
    roles:
      - chat
      - edit
      - apply
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 131072
      temperature: 0.2
      maxTokens: 8192

  - name: Mistral - Pixtral Large 2411 (Vision) - (128K)
    provider: mistral  # Use openai to bypass hardcoded limits
    model: pixtral-large-2411
    apiBase: https://api.mistral.ai/v1
    apiKey: ${MISTRAL_API_KEY}
    contextLength: 131072
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 131072
      temperature: 0.4
      maxTokens: 8192

  - name: Mistral - Ministral 8B (L√©ger) - (~32K)
    provider: mistral  # Use openai to bypass hardcoded limits
    model: ministral-8b-2410
    apiBase: https://api.mistral.ai/v1
    apiKey: ${MISTRAL_API_KEY}
    contextLength: 32768
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    defaultCompletionOptions:
      contextLength: 32768
      temperature: 0.5
      maxTokens: 8192

  # === MOD√àLES OPENROUTER (natifs et fonctionnels) ===
  # Mod√®le l√©ger natif OpenRouter
  - name: Openrouter - Aurora Alpha (L√©ger) - (128K)
    provider: openrouter
    model: openrouter/aurora-alpha
    apiBase: https://openrouter.ai/api/v1
    apiKey: ${OPENROUTER_API_KEY}
    contextLength: 128000
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 128000
      temperature: 0.2
      maxTokens: 8192

  # === MOD√àLES SILICONFLOW (API gratuite - hauts quotas) ===
  # Mod√®le Qwen avanc√© confirm√© pour MCP avec function calling
  - name: SiliconFlow - Qwen 3 32B - (131K)
    provider: siliconflow
    model: Qwen/Qwen3-32B
    apiBase: https://api.siliconflow.cn/v1
    apiKey: ${SILICONFLOW_API_KEY}
    contextLength: 131072
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      temperature: 0.2
      maxTokens: 8192

  # Mod√®le DeepSeek V3.2 confirm√© pour MCP avec function calling
  - name: SiliconFlow - DeepSeek V3.2 - (164K)
    provider: siliconflow
    model: deepseek-ai/DeepSeek-V3.2
    apiBase: https://api.siliconflow.cn/v1
    apiKey: ${SILICONFLOW_API_KEY}
    contextLength: 164000
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      temperature: 0.4
      maxTokens: 8192

  # === MOD√àLES GROQ (API gratuite - ultra-rapide) ===
  # Meilleur mod√®le Groq avec function calling et thinking
  - name: Groq - Groq Compound (Ultra-Rapide) - (131K)
    provider: groq
    model: groq/compound
    apiBase: https://api.groq.com/openai/v1
    apiKey: ${GROQ_API_KEY}
    contextLength: 131072
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    defaultCompletionOptions:
      contextLength: 131072
      temperature: 0.2
      maxTokens: 8192

  # Alternative rapide avec function calling
  - name: Groq - Qwen 3 32B - (131K)
    provider: groq
    model: qwen/qwen3-32b
    apiBase: https://api.groq.com/openai/v1
    apiKey: ${GROQ_API_KEY}
    contextLength: 131072
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    defaultCompletionOptions:
      contextLength: 131072
      temperature: 0.2
      maxTokens: 8192
      reasoning_format: parsed
      reasoning_effort: default

  # Mod√®le reasoning avanc√© avec contr√¥le d'effort
  - name: Groq - GPT-OSS 120B (Reasoning) - (131K)
    provider: groq
    model: openai/gpt-oss-120b
    apiBase: https://api.groq.com/openai/v1
    apiKey: ${GROQ_API_KEY}
    contextLength: 131072
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    defaultCompletionOptions:
      contextLength: 131072
      temperature: 0.2
      maxTokens: 8192
      reasoning_effort: medium
      include_reasoning: true
      reasoningBudgetTokens: 2048

  # === MOD√àLES CEREBRAS (API gratuite - function calling) ===
  # Mod√®le recommand√© pour MCP avec function calling et thinking
  - name: Cerebras - Qwen 3 235B - (65K)
    provider: cerebras
    model: qwen-3-235b-a22b-instruct-2507
    apiBase: https://api.cerebras.ai/v1
    apiKey: ${CEREBRAS_API_KEY}
    contextLength: 65536
    roles:
      - chat
      - edit
      - apply
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 65536
      temperature: 0.2
      maxTokens: 8192

  # Alternative puissante avec function calling
  - name: üî• Cerebras - GPT-OSS 120B - (65K)
    provider: cerebras
    model: gpt-oss-120b
    apiBase: https://api.cerebras.ai/v1
    apiKey: ${CEREBRAS_API_KEY}
    contextLength: 65536
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 65536
      temperature: 0.4
      maxTokens: 8192

  # GLM-5 remplace Pony Alpha (plus performant pour le codage)
  - name: Cerebras - GLM-4.7 (Codage) - (~64K)
    provider: cerebras
    model: zai-glm-4.7
    apiBase: https://api.cerebras.ai/v1
    apiKey: ${CEREBRAS_API_KEY}
    contextLength: 64000
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    autocompleteOptions:
      debounceDelay: 200
      maxPromptTokens: 1024
      onlyMyCode: true
    defaultCompletionOptions:
      contextLength: 64000
      temperature: 0.2
      maxTokens: 8192

  # === MOD√àLES GEMINI (API Google) ===
  # Mod√®le multimodal l√©ger et rapide
  - name: Gemini 2.5 Flash Lite (Multimodal) - Gemini (1M)
    provider: gemini
    model: gemini-2.5-flash-lite
    apiKey: ${GEMINI_API_KEY}
    contextLength: 1048576
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 1048576
      temperature: 0.3
      maxTokens: 8192

  # Mod√®le avanc√© avec features preview
  - name: Gemini 3 Flash Preview (Avanc√©) - Gemini (~1M)
    provider: gemini
    model: gemini-3-flash-preview
    apiKey: ${GEMINI_API_KEY}
    contextLength: 1048576
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 1048576
      temperature: 0.3
      maxTokens: 8192

  # Mod√®le avec grand contexte
  - name: Gemini 2.5 Flash (Grand Contexte) - Gemini (1M)
    provider: gemini
    model: gemini-2.5-flash
    apiKey: ${GEMINI_API_KEY}
    contextLength: 1048576
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 1048576
      temperature: 0.3
      maxTokens: 8192

  # Mod√®le Pro le plus performant
  - name: Gemini 2.5 Pro (Premium) - Gemini (1M)
    provider: gemini
    model: gemini-2.5-pro
    apiKey: ${GEMINI_API_KEY}
    contextLength: 1048576
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 1048576
      temperature: 0.3
      maxTokens: 8192

  # === MOD√àLES NVIDIA ===
  - name: üî• NVIDIA - Kimi K2.5 (256K)
    provider: nvidia
    model: moonshotai/kimi-k2.5
    apiBase: https://integrate.api.nvidia.com/v1
    apiKey: ${NVIDIA_API_KEY}
    contextLength: 262144
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 262144
      temperature: 0.6
      maxTokens: 8192
      chat_template_kwargs:
        thinking: true

  - name: üî• NVIDIA - Kimi K2 Thinking (Raisonnement) - (256K)
    provider: nvidia
    model: moonshotai/kimi-k2-thinking
    apiBase: https://integrate.api.nvidia.com/v1
    apiKey: ${NVIDIA_API_KEY}
    contextLength: 262144
    roles:
      - chat
      - edit
      - apply
      - autocomplete
    capabilities:
      - tool_use
    defaultCompletionOptions:
      contextLength: 262144
      temperature: 1.0
      maxTokens: 8192
      reasoningBudgetTokens: 2048

# --- CONTEXT PROVIDERS ---
context:
  - provider: file
  - provider: code
  - provider: diff
  # - provider: repo-map  # Commented out temporarily to reduce system message size (~36k tokens)
  #   params:
  #     includeSignatures: false

# --- MCP SERVERS (recherche agentique) ---
mcpServers:
  - name: filesystem
    command: npx
    args:
      - '-y'
      - '@modelcontextprotocol/server-filesystem'
      - /home/kidpixel
    description: Acc√®s aux fichiers pour lecture/√©criture
  - name: ripgrep
    command: npx
    args:
      - '-y'
      - 'mcp-ripgrep'
    description: Recherche textuelle rapide (grep) pour l'agent

# --- R√àGLES DE CONTEXTE ---
contextRules:
  # Fichiers markdown et documentation
  - pattern: '*.md'
    preferredModel: Mistral - Ministral 8B (L√©ger)
  - pattern: 'README*'
    preferredModel: Mistral - Ministral 8B (L√©ger)

  # Fichiers de code (priorit√© aux mod√®les avec grand contexte pour MCP)
  - pattern: '*.{py,js,ts,java,cpp,go,rs}'
    preferredModel: Gemini 2.5 Pro (Premium) - Gemini (1M)

  # Fichiers images (priorit√© aux mod√®les vision)
  - pattern: '*.{jpg,png,jpeg,gif,webp}'
    preferredModel: Mistral - Pixtral Large 2411 (Vision)

  # Fichiers de configuration
  - pattern: '*.{yaml,yml,toml,json}'
    preferredModel: Mistral - Mistral Large 2411 (Principal)