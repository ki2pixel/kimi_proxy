"""
memory.py - Routes API pour les op√©rations m√©moire (compression & similarit√©)

Pourquoi : Fournit les endpoints HTTP/WebSocket pour les fonctionnalit√©s
m√©moire avanc√©es du Kimi Proxy Dashboard
"""

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import asyncio
import logging
from datetime import datetime

from ...core.database import get_db
from ...core.tokens import count_tokens_tiktoken
from ...services.websocket_manager import ConnectionManager, get_connection_manager

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/memory", tags=["memory"])

# ============================================================================
# MOD√àLES PYDANTIC
# ============================================================================

class CompressionRequest(BaseModel):
    strategy: str = Field(..., description="Strat√©gie de compression", pattern="^(token|semantic)$")
    threshold: float = Field(..., ge=0.1, le=0.9, description="Seuil de compression")
    dry_run: bool = Field(default=False, description="Mode simulation uniquement")

class SimilarityRequest(BaseModel):
    reference_id: Optional[str] = Field(None, description="ID de la m√©moire de r√©f√©rence")
    reference_text: Optional[str] = Field(None, description="Texte de r√©f√©rence (alternative)")
    method: str = Field(default="cosine", description="M√©thode de similarit√©", pattern="^(cosine|jaccard|levenshtein)$")
    threshold: float = Field(default=0.75, ge=0.5, le=1.0, description="Seuil de similarit√©")
    limit: int = Field(default=20, ge=5, le=100, description="Nombre maximum de r√©sultats")

class MemoryItem(BaseModel):
    id: str
    title: str
    content: str
    content_preview: str
    type: str
    tokens: int
    created_at: datetime
    similarity_score: Optional[float] = None

class CompressionResult(BaseModel):
    success: bool
    current_count: int
    projected_count: int
    space_saved: str
    preview: List[Dict[str, Any]]
    compression_ratio: Optional[float] = None
    error: Optional[str] = None

class SimilarityResult(BaseModel):
    success: bool
    results: List[MemoryItem]
    total_found: int
    method_used: str
    threshold_used: float
    error: Optional[str] = None

# ============================================================================
# SERVICES M√âMOIRE
# ============================================================================

class MemoryService:
    """Service central pour les op√©rations m√©moire avec donn√©es r√©elles et similarit√© MCP"""
    
    def __init__(self, db, ws_manager: ConnectionManager):
        self.db = db
        self.ws_manager = ws_manager
        self._qdrant_client = None
        self._mock_memories = self._generate_mock_memories()  # Fallback uniquement
    
    def _get_qdrant_client(self):
        """Lazy loading du client Qdrant MCP"""
        if self._qdrant_client is None:
            try:
                from ...features.mcp.client import get_mcp_client
                client = get_mcp_client()
                if client and client.qdrant:
                    self._qdrant_client = client.qdrant
            except Exception as e:
                logger.warning(f"Qdrant client non disponible: {e}")
        return self._qdrant_client
    
    def _generate_mock_memories(self) -> List[Dict]:
        """G√©n√®re des m√©moires de test pour fallback si aucune donn√©e r√©elle"""
        memories = [
            {
                "id": "mem_001",
                "title": "Configuration FastAPI",
                "content": "Configuration du serveur FastAPI avec Uvicorn, middleware CORS, et routes API pour le proxy Kimi",
                "content_preview": "Configuration du serveur FastAPI avec Uvicorn...",
                "type": "configuration",
                "tokens": 156,
                "created_at": datetime.now()
            },
            {
                "id": "mem_002", 
                "title": "Architecture 5 Couches",
                "content": "Architecture en 5 couches: API ‚Üí Services ‚Üí Features ‚Üí Proxy ‚Üí Core avec d√©pendances unidirectionnelles",
                "content_preview": "Architecture en 5 couches: API ‚Üí Services ‚Üí Features...",
                "type": "architecture",
                "tokens": 234,
                "created_at": datetime.now()
            },
            {
                "id": "mem_003",
                "title": "Token Counting Tiktoken",
                "content": "Utilisation de tiktoken pour le comptage pr√©cis des tokens avec encodage cl100k_base",
                "content_preview": "Utilisation de tiktoken pour le comptage pr√©cis...",
                "type": "technique",
                "tokens": 189,
                "created_at": datetime.now()
            },
            {
                "id": "mem_004",
                "title": "WebSocket Manager",
                "content": "Gestionnaire WebSocket pour communication temps r√©el avec reconnexion automatique et diffusion d'√©v√©nements",
                "content_preview": "Gestionnaire WebSocket pour communication temps r√©el...",
                "type": "communication",
                "tokens": 267,
                "created_at": datetime.now()
            },
            {
                "id": "mem_005",
                "title": "MCP Phase 4 Integration",
                "content": "Int√©gration des 4 serveurs MCP Phase 4: Task Master, Sequential Thinking, Fast Filesystem, JSON Query",
                "content_preview": "Int√©gration des 4 serveurs MCP Phase 4...",
                "type": "integration",
                "tokens": 312,
                "created_at": datetime.now()
            }
        ]
        return memories
    
    async def _get_real_memories_from_db(self, limit: int = 50) -> List[Dict]:
        """R√©cup√®re les m√©moires r√©elles depuis la base de donn√©es SQLite"""
        try:
            cursor = self.db.cursor()
            cursor.execute("""
                SELECT id, session_id, memory_type, content_hash, content_preview, 
                       full_content, token_count, access_count, created_at, metadata
                FROM mcp_memory_entries
                ORDER BY access_count DESC, last_accessed_at DESC
                LIMIT ?
            """, (limit,))
            
            rows = cursor.fetchall()
            memories = []
            
            for row in rows:
                mem = {
                    "id": str(row[0]),
                    "title": row[4][:50] if row[4] else f"M√©moire {row[0]}",
                    "content": row[5] or row[4] or "",
                    "content_preview": row[4] or "",
                    "type": row[2] or "episodic",
                    "tokens": row[6] or 0,
                    "created_at": datetime.fromisoformat(row[8].replace('Z', '+00:00')) if row[8] and isinstance(row[8], str) else datetime.now(),
                    "session_id": row[1],
                    "access_count": row[7] or 0
                }
                memories.append(mem)
            
            return memories
            
        except Exception as e:
            logger.error(f"Erreur r√©cup√©ration m√©moires DB: {e}")
            return []
    
    async def get_frequent_memories(self) -> List[MemoryItem]:
        """R√©cup√®re les m√©moires fr√©quentes depuis la base de donn√©es"""
        real_memories = await self._get_real_memories_from_db(limit=50)
        
        # Fallback vers mock si aucune donn√©e r√©elle
        if not real_memories:
            logger.info("Aucune m√©moire r√©elle trouv√©e, utilisation du fallback mock")
            real_memories = self._mock_memories
        
        return [MemoryItem(**mem) for mem in real_memories]
    
    async def preview_compression(self, strategy: str, threshold: float) -> CompressionResult:
        """Pr√©visualise la compression m√©moire avec donn√©es r√©elles"""
        try:
            # R√©cup√®re les m√©moires r√©elles
            memories = await self._get_real_memories_from_db(limit=100)
            
            # Fallback vers mock si aucune donn√©e r√©elle
            if not memories:
                memories = self._mock_memories
            
            # Simulation de l'analyse
            await asyncio.sleep(0.1)  # R√©duit pour meilleure UX
            
            total_tokens = sum(m["tokens"] for m in memories)
            
            # Strat√©gie token: fusionner les petites m√©moires
            if strategy == "token":
                small_memories = [m for m in memories if m["tokens"] < threshold * 1000]
                projected_count = len(memories) - len(small_memories) // 2
                space_saved_tokens = sum(m["tokens"] for m in small_memories) // 2
                
                preview = [
                    {
                        "action": "fusionner",
                        "items": f"{len(small_memories)} petites m√©moires",
                        "tokens": space_saved_tokens,
                        "preview": f"Fusion de {len(small_memories)} m√©moires < {threshold * 1000:.0f} tokens"
                    }
                ]
            else:  # semantic
                # Regroupement s√©mantique bas√© sur les types de m√©moires
                types = {}
                for m in memories:
                    t = m.get("type", "unknown")
                    types[t] = types.get(t, 0) + 1
                
                dominant_type = max(types.items(), key=lambda x: x[1]) if types else ("unknown", 0)
                projected_count = max(2, len(memories) - len(types) + 1)
                space_saved_tokens = int(total_tokens * 0.25)  # Estimation 25% √©conomie
                
                preview = [
                    {
                        "action": "regrouper",
                        "items": f"{len(types)} types de m√©moires",
                        "tokens": space_saved_tokens,
                        "preview": f"Regroupement par similarit√© s√©mantique (type dominant: {dominant_type[0]})"
                    }
                ]
            
            return CompressionResult(
                success=True,
                current_count=len(memories),
                projected_count=projected_count,
                space_saved=f"{space_saved_tokens} tokens",
                preview=preview,
                compression_ratio=space_saved_tokens / total_tokens if total_tokens > 0 else 0
            )
            
        except Exception as e:
            logger.error(f"Erreur preview compression: {e}")
            return CompressionResult(
                success=False,
                current_count=0,
                projected_count=0,
                space_saved="0 tokens",
                preview=[],
                error=str(e)
            )
    
    async def execute_compression(self, strategy: str, threshold: float, dry_run: bool) -> CompressionResult:
        """Ex√©cute la compression m√©moire avec donn√©es r√©elles"""
        if dry_run:
            return await self.preview_compression(strategy, threshold)
        
        try:
            # R√©cup√®re les m√©moires r√©elles
            memories = await self._get_real_memories_from_db(limit=100)
            
            # Fallback vers mock si aucune donn√©e r√©elle
            if not memories:
                memories = self._mock_memories
            
            # Simulation de l'ex√©cution (optimis√©e)
            await asyncio.sleep(0.2)
            
            # Log la compression dans la base de donn√©es
            try:
                cursor = self.db.cursor()
                for mem in memories[:5]:  # Limite pour performance
                    cursor.execute("""
                        INSERT INTO compression_log 
                        (session_id, original_tokens, compressed_tokens, compression_ratio, summary_preview)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        mem.get("session_id", 0),
                        mem["tokens"],
                        int(mem["tokens"] * 0.8),  # Simulation 20% compression
                        0.2,
                        mem["content_preview"][:100]
                    ))
                self.db.commit()
            except Exception as e:
                logger.warning(f"Erreur logging compression: {e}")
            
            # Notifier les clients WebSocket
            await self.ws_manager.broadcast({
                "type": "memory_compression_completed",
                "strategy": strategy,
                "threshold": threshold,
                "memories_processed": len(memories),
                "timestamp": datetime.now().isoformat()
            })
            
            return await self.preview_compression(strategy, threshold)
            
        except Exception as e:
            logger.error(f"Erreur ex√©cution compression: {e}")
            return CompressionResult(
                success=False,
                current_count=0,
                projected_count=0,
                space_saved="0 tokens",
                preview=[],
                error=str(e)
            )
    
    async def find_similar_memories(self, request: SimilarityRequest) -> SimilarityResult:
        """Recherche des m√©moires similaires avec Qdrant MCP ou algorithmes locaux"""
        try:
            # R√©cup√®re les m√©moires r√©elles
            memories = await self._get_real_memories_from_db(limit=100)
            
            # Fallback vers mock si aucune donn√©e r√©elle
            if not memories:
                memories = self._mock_memories
            
            # D√©termine le texte de r√©f√©rence
            reference_text = request.reference_text
            if request.reference_id and not reference_text:
                # Cherche la m√©moire par ID
                for mem in memories:
                    if mem["id"] == request.reference_id:
                        reference_text = mem["content"]
                        break
            
            if not reference_text:
                return SimilarityResult(
                    success=False,
                    results=[],
                    total_found=0,
                    method_used=request.method,
                    threshold_used=request.threshold,
                    error="Texte de r√©f√©rence non trouv√©"
                )
            
            # Essaie d'abord Qdrant si disponible
            qdrant_client = self._get_qdrant_client()
            if qdrant_client:
                try:
                    # V√©rifie si Qdrant est disponible
                    status = await qdrant_client.check_status()
                    if status and status.connected:
                        logger.info(f"üîç Utilisation de Qdrant pour similarit√© (latence: {status.latency_ms:.1f}ms)")
                        
                        # Recherche s√©mantique via Qdrant
                        qdrant_results = await qdrant_client.search_similar(
                            query=reference_text[:500],  # Limite pour performance
                            limit=request.limit,
                            score_threshold=request.threshold
                        )
                        
                        # Mappe les r√©sultats Qdrant vers MemoryItem
                        results = []
                        for hit in qdrant_results:
                            mem_item = MemoryItem(
                                id=hit.id,
                                title=hit.content_preview[:50] if hit.content_preview else f"R√©sultat {hit.id}",
                                content=hit.full_content or hit.content_preview or "",
                                content_preview=hit.content_preview or "",
                                type=hit.metadata.get("type", "semantic") if hit.metadata else "semantic",
                                tokens=hit.metadata.get("token_count", 0) if hit.metadata else 0,
                                created_at=datetime.now(),  # Qdrant ne stocke pas le datetime
                                similarity_score=hit.score
                            )
                            results.append(mem_item)
                        
                        return SimilarityResult(
                            success=True,
                            results=results,
                            total_found=len(results),
                            method_used="qdrant_semantic",
                            threshold_used=request.threshold
                        )
                except Exception as e:
                    logger.warning(f"Qdrant indisponible, fallback vers algorithmes locaux: {e}")
            
            # Fallback: Algorithmes locaux
            logger.info(f"üî¢ Utilisation des algorithmes locaux ({request.method})")
            results = await self._find_similar_local(
                memories, reference_text, request.method, request.threshold, request.limit
            )
            
            return SimilarityResult(
                success=True,
                results=results,
                total_found=len(results),
                method_used=request.method,
                threshold_used=request.threshold
            )
            
        except Exception as e:
            logger.error(f"Erreur recherche similarit√©: {e}")
            return SimilarityResult(
                success=False,
                results=[],
                total_found=0,
                method_used=request.method,
                threshold_used=request.threshold,
                error=str(e)
            )
    
    async def _find_similar_local(
        self, 
        memories: List[Dict], 
        reference_text: str, 
        method: str, 
        threshold: float, 
        limit: int
    ) -> List[MemoryItem]:
        """Algorithmes de similarit√© locaux (fallback)"""
        
        # Normalise le texte de r√©f√©rence
        ref_words = self._extract_words(reference_text)
        ref_set = set(ref_words)
        
        results = []
        for mem in memories:
            mem_words = self._extract_words(mem["content"])
            mem_set = set(mem_words)
            
            # Calcule le score selon la m√©thode
            if method == "cosine":
                score = self._cosine_similarity(ref_words, mem_words)
            elif method == "jaccard":
                score = self._jaccard_similarity(ref_set, mem_set)
            else:  # levenshtein
                score = self._levenshtein_similarity(reference_text, mem["content"])
            
            # Filtre par seuil
            if score >= threshold:
                mem_copy = mem.copy()
                mem_copy["similarity_score"] = round(score, 3)
                results.append(MemoryItem(**mem_copy))
        
        # Trie par score d√©croissant
        results.sort(key=lambda x: x.similarity_score, reverse=True)
        
        # Limite les r√©sultats
        return results[:limit]
    
    def _extract_words(self, text: str) -> List[str]:
        """Extrait les mots d'un texte (tokenisation simple)"""
        if not text:
            return []
        # Minuscules, alphanum√©rique uniquement, mots > 2 caract√®res
        words = []
        for word in text.lower().split():
            clean = ''.join(c for c in word if c.isalnum())
            if len(clean) > 2:
                words.append(clean)
        return words
    
    def _cosine_similarity(self, words1: List[str], words2: List[str]) -> float:
        """Calcule la similarit√© cosinus entre deux listes de mots"""
        if not words1 or not words2:
            return 0.0
        
        # Compte les fr√©quences
        from collections import Counter
        vec1 = Counter(words1)
        vec2 = Counter(words2)
        
        # Vocabulaire commun
        all_words = set(vec1.keys()) | set(vec2.keys())
        
        # Vecteurs
        v1 = [vec1.get(w, 0) for w in all_words]
        v2 = [vec2.get(w, 0) for w in all_words]
        
        # Produit scalaire
        dot_product = sum(a * b for a, b in zip(v1, v2))
        
        # Normes
        norm1 = sum(a * a for a in v1) ** 0.5
        norm2 = sum(b * b for b in v2) ** 0.5
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _jaccard_similarity(self, set1: set, set2: set) -> float:
        """Calcule le coefficient de similarit√© de Jaccard"""
        if not set1 and not set2:
            return 1.0
        if not set1 or not set2:
            return 0.0
        
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        return intersection / union if union > 0 else 0.0
    
    def _levenshtein_similarity(self, s1: str, s2: str) -> float:
        """Calcule la similarit√© bas√©e sur la distance de Levenshtein (normalis√©e)"""
        if not s1 and not s2:
            return 1.0
        if not s1 or not s2:
            return 0.0
        
        # Distance de Levenshtein
        distance = self._levenshtein_distance(s1[:200], s2[:200])  # Limite pour performance
        max_len = max(len(s1), len(s2))
        
        return 1.0 - (distance / max_len) if max_len > 0 else 0.0
    
    def _levenshtein_distance(self, s1: str, s2: str) -> int:
        """Calcule la distance de Levenshtein entre deux cha√Ænes (programmation dynamique)"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        # Optimisation: utilise deux lignes au lieu de matrice compl√®te
        previous_row = list(range(len(s2) + 1))
        current_row = [0] * (len(s2) + 1)
        
        for i, c1 in enumerate(s1):
            current_row[0] = i + 1
            
            for j, c2 in enumerate(s2):
                # Co√ªts: insertion, deletion, substitution
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row[j + 1] = min(insertions, deletions, substitutions)
            
            # Swap rows
            previous_row, current_row = current_row, previous_row
        
        return previous_row[len(s2)]

# ============================================================================
# ENDPOINTS API
# ============================================================================

@router.get("/frequent", response_model=List[MemoryItem])
async def get_frequent_memories(db=Depends(get_db)):
    """R√©cup√®re les m√©moires fr√©quentes depuis la base de donn√©es"""
    ws_manager = get_connection_manager()
    service = MemoryService(db, ws_manager)
    return await service.get_frequent_memories()

@router.post("/compress/preview", response_model=CompressionResult)
async def preview_compression(
    strategy: str,
    threshold: float,
    db=Depends(get_db)
):
    """Pr√©visualise la compression m√©moire"""
    ws_manager = get_connection_manager()
    service = MemoryService(db, ws_manager)
    return await service.preview_compression(strategy, threshold)

@router.post("/compress/execute", response_model=CompressionResult)
async def execute_compression(
    request: CompressionRequest,
    db=Depends(get_db)
):
    """Ex√©cute la compression m√©moire"""
    ws_manager = get_connection_manager()
    service = MemoryService(db, ws_manager)
    return await service.execute_compression(
        request.strategy, 
        request.threshold, 
        request.dry_run
    )

@router.post("/similarity/search", response_model=SimilarityResult)
async def search_similar_memories(
    request: SimilarityRequest,
    db=Depends(get_db)
):
    """Recherche des m√©moires similaires"""
    ws_manager = get_connection_manager()
    service = MemoryService(db, ws_manager)
    return await service.find_similar_memories(request)

@router.get("/stats")
async def get_memory_stats(db=Depends(get_db)):
    """Statistiques sur les m√©moires"""
    ws_manager = WebSocketManager()
    service = MemoryService(db, ws_manager)
    memories = await service.get_frequent_memories()
    
    total_tokens = sum(m.tokens for m in memories)
    avg_tokens = total_tokens / len(memories) if memories else 0
    
    return {
        "total_memories": len(memories),
        "total_tokens": total_tokens,
        "average_tokens": round(avg_tokens, 1),
        "types": list(set(m.type for m in memories)),
        "last_updated": datetime.now().isoformat()
    }

# ============================================================================
# HANDLERS WEBSOCKET
# ============================================================================

async def handle_memory_compress_preview(websocket, data: Dict[str, Any]):
    """Handler WebSocket pour preview compression"""
    try:
        db = get_db()
        ws_manager = get_connection_manager()
        service = MemoryService(db, ws_manager)
        
        result = await service.preview_compression(
            data.get("strategy", "token"),
            data.get("threshold", 0.3)
        )
        
        await websocket.send_json({
            "type": "memory_compress_preview_response",
            "requestId": data.get("requestId"),
            "result": result.dict()
        })
        
    except Exception as e:
        logger.error(f"Erreur WebSocket preview compression: {e}")
        await websocket.send_json({
            "type": "memory_compress_preview_response",
            "requestId": data.get("requestId"),
            "error": str(e)
        })

async def handle_memory_compress_execute(websocket, data: Dict[str, Any]):
    """Handler WebSocket pour ex√©cution compression"""
    try:
        db = get_db()
        ws_manager = get_connection_manager()
        service = MemoryService(db, ws_manager)
        
        result = await service.execute_compression(
            data.get("strategy", "token"),
            data.get("threshold", 0.3),
            data.get("dryRun", False)
        )
        
        await websocket.send_json({
            "type": "memory_compress_result_response",
            "requestId": data.get("requestId"),
            "result": result.dict()
        })
        
    except Exception as e:
        logger.error(f"Erreur WebSocket ex√©cution compression: {e}")
        await websocket.send_json({
            "type": "memory_compress_result_response",
            "requestId": data.get("requestId"),
            "error": str(e)
        })

import json
from datetime import datetime

# Helper pour s√©rialiser les objets datetime
def serialize_datetime(obj):
    if isinstance(obj, datetime):
        return obj.isoformat()
    raise TypeError(f"Object of type {type(obj)} is not JSON serializable")

async def handle_memory_similarity_search(websocket, data: Dict[str, Any]):
    """Handler WebSocket pour recherche similarit√©"""
    try:
        db = get_db()
        ws_manager = get_connection_manager()
        service = MemoryService(db, ws_manager)
        
        request = SimilarityRequest(**data.get("payload", {}))
        result = await service.find_similar_memories(request)
        
        # S√©rialise correctement avec gestion des datetime
        response_data = {
            "type": "memory_similarity_result_response",
            "requestId": data.get("requestId"),
            "result": json.loads(json.dumps(result.dict() if hasattr(result, 'dict') else result, default=serialize_datetime))
        }
        
        await websocket.send_json(response_data)
        
    except Exception as e:
        logger.error(f"Erreur WebSocket recherche similarit√©: {e}")
        await websocket.send_json({
            "type": "memory_similarity_result_response",
            "requestId": data.get("requestId"),
            "error": str(e)
        })

# Export des handlers pour le WebSocket manager
WEBSOCKET_HANDLERS = {
    "memory_compress_preview": handle_memory_compress_preview,
    "memory_compress_execute": handle_memory_compress_execute,
    "memory_similarity_search": handle_memory_similarity_search,
}